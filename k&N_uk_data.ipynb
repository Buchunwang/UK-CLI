{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCQc0JONW5oQbzpgJ+YTOi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Buchunwang/UK-CLI/blob/main/k%26N_uk_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**This code is replication of Kim&Nelson (1998) with UK data**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Brief introduction of the code:**\\\n",
        "This is a Bayesian algorithm with three main blocks:\n",
        "\n",
        "1. Kalman filter\n",
        "2. Hamilton(1989) basic filter\n",
        "3. Estiamting parameters by multi-move Gibbs sampling.\n",
        "\n",
        "We iteratively run the process 1→2→3→1→2→3→1…\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The execution of this code takes a few hours, so it is recommended to run it on your own device.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "From the algorithm, we can generate:\n",
        "1. a new economic index of UK\n",
        "2. historical recession period of UK\n",
        "3.In the end, we will also display the distribution of all the parameters.\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JOqCqtSDrD5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data**\\\n",
        "First three columns are first order of original UK composite leading indicator(CLI) with 0 lag, 1 lag and 2 lags: $\\Delta c_t$, $\\Delta c_{t-1}$ , $\\Delta c_{t-2}$.\\\n",
        "For column 4:8, they are component variables $\\Delta y_i, i=1,2,3,4,5$ applied to generate original CLI.\\\n",
        " There were 6 component variables in totoal, but we removed new driver number every year as the data is not suitable for this model without etxra modificaions.\\\n",
        "Column 9:13,13:18 are component variables with 1 lag and 2 lags individually:$\\Delta y_{i_{t-1}} \\& \\ \\Delta y_{i_{t-2}}, i=1,2,3,4,5$.\n",
        "\n",
        "All data imported have been detrended.\n",
        "\n",
        "Name of $Y_i$ :\n",
        "\n",
        "1. Retail Prices Index - All Items Index\n",
        "2. Consumer survey - confidence indicator\n",
        "3.  GBP interbank LIBOR 3 months delayed of UK  \n",
        "4.  Manufacturing survey - production: future tendency sa (season adjusted)\n",
        "5.  Share prices: FTSE LOCAL UK ($£ $) index\n",
        "\n",
        "Data source: OECD\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iasF8HjXwH_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE2WbK2XnZ63",
        "outputId": "e6de1c74-b3d3-4ad8-e3d0-7f2118e5ce84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From URL: https://raw.githubusercontent.com/Buchunwang/UK-CLI/main/Kalman%20filter/kalpre1.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "url = 'https://raw.githubusercontent.com/Buchunwang/UK-CLI/main/Kalman%20filter/kalpre1.csv'\n",
        "print('From URL:', url)\n",
        "kalpre1 = pd.read_csv(url, header=None, encoding='utf-8', skiprows=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "1.\n",
        "\\begin{equation}\n",
        "\\Delta Y_{it}=\\lambda_i(L)\\Delta C_t+D_i+e_{it} \\ \\ \\ i=1,2,3,4,5 \\ \\ \\ t=1,2… T\n",
        "\\end{equation}\n",
        "$Y_{it}$ are component series above except passenger car registrations,$C_t$ is the growth rate of CLI,\n",
        "$\\lambda _i(L)=\\lambda _i $ here for simplicity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.\n",
        "\n",
        "\\begin{equation}\n",
        "\\Psi_i(L)e_{it}=\\epsilon_{it},  \\ \\ \\ \\epsilon_{it}\\sim iid N(0,\\sigma_i^2)\t  \n",
        "\\end{equation}\n",
        "where $\\Psi_i(L)=1-\\psi_{i1}L-\\psi_{i2}L^2$, $L$ is the lag operator\n",
        "\n",
        "3.\n",
        "\\begin{equation}\n",
        "\\Phi(L)(\\Delta C_t-\\mu_{s_t}-\\delta\n",
        ")=v_t\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ v_t\\sim iid N(0,σ^2_{vt})\n",
        "\\end{equation}\n",
        "where $\\Phi(L)=1-\\phi_1L-\\phi_2L^2$. $\\mu_{s_t}-\\delta$ is the mean growth rate of $\\Delta C_t$, $\\mu_{s_t}$ is the regime-switching component.\n",
        "\n",
        "\n",
        "\n",
        "4.\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\mu_{s_t}=\\mu_0+\\mu_1S_t\n",
        "\\end{equation}\n",
        "where $S_t=\\{\\begin{matrix}\n",
        "\t\t0,1\n",
        "\t\\end{matrix}\\}$\n",
        " and $\\mu_1<0$.\n",
        "The regime-switching happens at the mean value of $\\Delta C_t$.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We remove the mean value of $\\Delta Y_{it}$ and $\\Delta C_t$:\n",
        "\\begin{equation}\n",
        "\\Delta c_t=\\Delta C_t-\\delta\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\Delta y_{it}=\\Delta Y_{it}-\\Delta \\overline{Y_i}\n",
        "\\end{equation}\n",
        "\n",
        "---\n",
        "\n",
        "Equation 1 and 3 can then be replaced by\n",
        "\n",
        "5.\n",
        "\\begin{equation}\n",
        "\\Delta y_{it}=\\lambda_i(L)\\Delta c_t+e_{it} \\ \\ \\ i=1,2,3,4 \\ \\ \\ t=1,2… T\n",
        "\\end{equation}\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "6.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\Phi(L)(\\Delta c_t-\\mu_{s_t}\n",
        ")=v_t\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ v_t\\sim iid N(0,1)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "E04jRgUJEsLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are 4 functions we are going to use:\n",
        "1.  Kalman Filter (regime-switching state-space Kalman filter)\n",
        "2.  Update process afetr Kalman filter using t+1 data for time t\n",
        "2.  Posterior normal distribution\n",
        "3.  Posterior inverse Gamma distribution"
      ],
      "metadata": {
        "id": "oZFImVt8mwEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Function 1: regime-switching Kalman filter\\\n",
        "input:\n",
        "1.  parameters generated from last step's multimove Gibbd sampling\n",
        "including $\\Phi(L), \\Psi_i(L), \\mu_0,\\mu_1,\\sigma_i, \\lambda_i(L), i= 1,2,3,4$\n",
        "2.  series $\\Delta y_{it} , i=1,2,3,4$\n",
        "\n",
        "output:\n",
        "1. mean & variance of $\\Delta \\zeta_{t|t} , t=1, ⋯⋯, T-1, T$\n",
        "2. steady-state Kalman gain\n",
        "\n",
        "\n",
        "---\n",
        "\\begin{equation}\n",
        "\t\\Delta y^*_t=H^*\\zeta_t+\\epsilon_t\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\t\\zeta_t=\\tilde M^*_{s_t}+F^*\\zeta_{t-1}+u^*_t\n",
        "\\end{equation}\n",
        "\n",
        "---\n",
        "For the fuction, we have 9 input:\n",
        "1. N: length of input data\n",
        "2. true4D: the input time series data with 4 dimension, which is $\\Delta y^*_t$ in above equation\n",
        "3. R: measurement noise covariance martix with variance of $\\epsilon_{it}$ on the diagonal\n",
        "$R=\\begin{bmatrix}\n",
        "\\sigma^2_1&0&0&0&0\\\\\n",
        "0&\\sigma^2_2&0&0&0\\\\\n",
        "0&0& \\sigma^2_3&0&0\\\\\n",
        "0&0&0&\\sigma^2_4&0\\\\0\n",
        "&0&0&0&\\sigma^2_5\n",
        "\\end{bmatrix}\n",
        "$\n",
        "4. Q_kal: the covariance matrix of $u^*_t$, the size of Q_kal depends on the length of vector $\\zeta_t$. if $\\zeta_t$ is a vector with k columns, then size of Q_kal is $k\\times k$.\n",
        "5. P: the covariance of $\\zeta_t$. P is updated during the process. We can initialize P as an identity matrix or a zero matrix; alternatively, we can choose a more appropriate initialization to provide the Kalman filter with a better prior.\n",
        "6. H: measurement matrix. It maps the state variables into the measured outputs.\n",
        "7. A (or F in the equation above): state transition matrix, it connects the two vector by $\\xi_t=F^*\\zeta_{t-1}+Q_{kal}$\n",
        "8. K: Kalman gain, this is just initialization, K would get updated in the process. We can set K as a zeros matrix with the same size as H.T (transposed H)\n",
        "9. Mu_st: vector of state-dependent element, correspondent with $\\tilde M^*_{s_t}$ in the equation\n",
        "\n",
        "About $\\zeta_t$: This is unknown. Mean and variance of $\\xi_t$ is what we are going to estimate through Kalman filter.\n",
        "\n",
        "Also: not all element inside the $\\xi_t$ are going to be used in the whole iteration. $\\xi_t$ might be a vector such as $\\zeta_t=\\begin{bmatrix}\n",
        "\t\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2}\n",
        "\\end{bmatrix}$, but in the end we only need $\\Delta c_t$ which is the first element. This will be given more detailed explainations in the full algorithm part with examples.\n",
        "\n",
        "Before running Kalman filter, we should also prepare empty matrix to save the output which is the mean and variance of $\\zeta_t$ ( or some specific rows of $\\zeta_t$).\n",
        "\n",
        "\n",
        "---\n",
        "Kalman filter has 5 equations: First 2 equations are called the process of predicting, later 3 equations are called the proecess of filtering or correction.\n",
        "\n",
        "Initial values are required for the first iteration of kalman filter. For the first input $t=1$, we assume mean value $\\zeta_{1|1}=0$ and variance $P_{1|1}$ be a diagonal matrix with 0.1 on the diagonals. Choice of initial value depends on the user but it would be better to set some reasonable initial. Very wrong initialization could cause trouble for other blocks of the MCMC algorithm in actual tests.\n",
        "\n",
        "\n",
        "t iterates from t=2 to t=T, we should provide the\n",
        "\n",
        "For each iteration t, we use t-1 as input. $\\zeta_{t|t}$ from last iteration would become $\\zeta_{t-1|t-1}$ for next iteration. $P_{t|t}$ from last iteration would become $P_{t-1|t-1}$ for next iteration.\n",
        "\n",
        "---\n",
        "\n",
        "Kalman equation 1:\n",
        "\\begin{equation}\n",
        " \\zeta_{t|t-1}=F^*\\zeta_{t-1|t-1}+\\tilde M^*_{s_t}\n",
        " \\end{equation}\n",
        " Equation 1 is the process of **predicting** mean value of $\\zeta_t $ from $\\zeta_{t-1}$.  $\\  \\zeta_{t|t-1}=E[\\zeta_t|t-1]=E[F*\\zeta_{t-1}+\\phi(L)\\mu_{st}+v_t|t-1]=F*\\zeta_{t-1}$ as $\\mu_{st}$ is imposed mean zero in the setting according to Kim and Nelson (1998) and $v_t$ is iid noise of time t.\n",
        "\n",
        "   This is extended version of normal state-space Kalman filter as we added state-dependent element $M_{st}$ in the first kalman filter equation. For normal kalman filter, first Kalman equation is $\\zeta_{t|t-1}=F^*\\zeta_{t-1|t-1}$.\n",
        "\n",
        "---  \n",
        "Kalman equation 2:\n",
        " \\begin{equation}\n",
        " P_{t|t-1}=F^*P_{t-1|t-1}F^{*^T}+Q\n",
        " \\end{equation}\n",
        " Equation 2 is the process of **predicting** variance of $\\zeta_t $ from $\\zeta_{t-1}$. $Q$ is the variance matrix of process noise $[v_t,\\ v_{t-1},\\ 0]'$.  \n",
        "\n",
        "---\n",
        "Kalman equation 3:\n",
        "\\begin{equation}\n",
        "\tK(t)= P_{t|t-1}H^{*^T}[H^*P_{t|t-1}H^{*^T}+R]^{-1}      \n",
        "\\end{equation}\n",
        "Equation 3 is the process of **filtering** or upgrading. Through this equation we can obtain the value of Kalman gain $K$. Kalman gain would stablize after a few iterations (which is called as steady-state Kalman gain). This is caused by the stablized variance matrix $P$ after a few iterations.\n",
        "  \n",
        "---\n",
        "Kalman equation 4:\n",
        "\\begin{equation}\n",
        " \\zeta_{t|t}= \\zeta_{t|t-1}+K(t)([\\Delta y^*_t-H^*\\zeta_{t|t-1}]\n",
        "\\end{equation}\n",
        "Equation 4 is the process of **filtering** mean value of $\\zeta_t$ by adding the filtered error term at time t.\n",
        "  \n",
        "---\n",
        "Kalman equation 5:\n",
        "\\begin{equation}\n",
        "\tP_{t|t}=[I-K(t)H^*]P_{t|t-1}\n",
        "\\end{equation}\n",
        "where $I$ is identity matrix.  \n",
        "Equation 5 is the process of **filtering** variance of $\\zeta_t$. $P$ would stablize with the stablization of Kalman gain after a few iterations.\n",
        "  "
      ],
      "metadata": {
        "id": "indl-tDURveK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "Function 2: the process of adding t+1 data to the information set after running kalman filter\n",
        "\n",
        "This process is from step 2 of appedix A1 in Kim $\\&$ Nelson (1998). After running the Kalman filter to generate mean and variance of $\\Delta c_t$ conditional on t, which is $\\Delta c_{t|t}$, Kim applied t+1 data to update the mean & variance of $\\Delta c_{t|t}$.\n",
        "\n",
        "Reason for using t+1 data to improve estimation:\n",
        "1.  From information set side, for data at time t, as Kalman filter is a forward iteration, data from 1 to t-1 are already in the information set for $\\Delta c_{t|t}$. If we want something new, some extra information, it's only available from the rightside of t: t+1...,T-1,T. Considering t+1 data is known from Kalman filter, we can now treat t+1 data as observations.\n",
        "2.  From Kalman filter side, kalman filter would smooth the series and reduce the noise, but at the same time, kalman filter would make the series generated from $\\Delta y^*_t$ less informative. This might be kind of vague, one example is: If we don't do this process after the Kalman filter, the sharp decrease in index caused by great recession (such as 2008, 2020) would get smoothed by Kalman filter. It would make 2008 or 2020 look like relatively mild recessions which is not proper. This 't+1' process can effectively bring back the useful information (such as strength of recession) to the generated series $\\Delta c_t$.\n",
        "3.  From final effect side, the variance of each data point of $\\Delta c_t$ would decrease after this process because the whole system gets more informative after including t+1 data as extra observations into information set. The final sample of $\\Delta c_{t|t+1}$ generated from new mean and variance would be less noisy compared with $\\Delta c_{t|t}$ generated from former mean and variance.\n",
        "\n",
        "---\n",
        "Reference for this updating process:chapter 13.6 http://mayoral.iae-csic.org/timeseries2021/hamilton.pdf  or see step2 of appendix A2 from Kim and Nelson (1998)\n",
        "\n",
        "---\n",
        "Update equations:\n",
        "\\begin{equation}\n",
        "\t\\zeta_{t|t,\\Delta c_{t+1}}=\\zeta_{t|t}+V_{t|t}F^*(1)\\eta_t/R_t\n",
        "\\end{equation}  \n",
        "\n",
        "\\begin{equation}\n",
        "\tV_{t|t,\\Delta c_{t+1}}=V_{t|t}-V_{t|t}F^*(1)F^*(1)'V_{t|t}'/R_t\n",
        "\\end{equation}\n",
        "\n",
        "where $\\eta_t=\\Delta c_{t+1}-\\Phi(L)\\mu_{t+1}-F^*(1)\\zeta_{t|t}$ and $R_t=F^*(1)V_{t|t}F^*(1)'+var[u^*_{t+1}(1)]$. $F^*(1)$ and $u^*_{t+1}(1)$ are first row of $F^*$ and $u^*_{t+1}$.\n",
        "$ F^* =\\begin{bmatrix}\n",
        "\t\t\\phi_1 & \\phi_2 & 0  \\\\\n",
        "1 & 0 & 0\\\\\n",
        "0 & 1 & 0\n",
        "\t\\end{bmatrix}\n",
        "$, so $F^*(1)=\\begin{bmatrix}\n",
        "\t\t\\phi_1 & \\phi_2 & 0\n",
        "   \\end{bmatrix}$\n",
        "       \n",
        "       \n",
        "$\\zeta_{t|t}=\\begin{bmatrix}\n",
        "\t\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2}\n",
        "\\end{bmatrix}$\n",
        "$\n",
        "V_{t|t}=\\begin{bmatrix}\n",
        "\tVar(\\Delta c_t)\\\\ Var(\\Delta c_{t-1}) \\\\ Var(\\Delta c_{t-2})\n",
        "\\end{bmatrix}\n",
        "$\n",
        "$var[u^*_{t+1}(1)]=var(v_{t+1})=1$ as $v_t\\sim iid N(0,1)$\n"
      ],
      "metadata": {
        "id": "Vm4RD6ffSW_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "Function 3:calculating posterior mean & variance for normal distribution\n",
        "\n",
        "---\n",
        "Input:\n",
        "prior mean & variance (fixed during the whole process of MCMC)\n",
        "a & b: these two depends on the specific setting of each parameter\n",
        "\n",
        "Output:\n",
        "posterior mean & variance of the parameter\n",
        "\n",
        "---\n",
        "posterior mean = ((prior var$)^{-1}$+a$)^{-1}* $ ((prior var$)^{-1}*$ (prior mean) + b )\n",
        "\n",
        "posterior var  = ((prior var$)^{-1}$+a$)^{-1}$\n",
        "\n",
        "---\n",
        "In the code, we wrote seperate code for mean & variance in scalar and matrix form. The math part is same but we would use inverse matrix function which doesn't fix scalars."
      ],
      "metadata": {
        "id": "LSnWgBBaSXI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        " Function 4: generating sample of variance from posterior inverse distribution\n",
        "\n",
        "---\n",
        "Input: a & b\n",
        "\n",
        "a is Shape Parameter : It influences the shape of the distribution. Higher values of a will make the distribution peak more to the right of the origin, concentrating more probability mass on higher values.\n",
        "\n",
        "b is Scale Parameter: It is a positive scalar that stretches or shrinks the distribution along the x-axis. Larger values of b will spread the distribution out, placing more probability mass over a wider range of values.\n",
        "\n",
        "Here our input a & b are posterior for inverse gamma distribution.\n",
        "\n",
        "Output: Variance samples generated from the posterior inverse gamma distribution:In the implementation, if we opt to use the gamma distribution to generate random samples, the function should be represented as $ 1/\\text{IG}(a1, 1/b1) $ for $ \\sigma^2 \\sim \\text{IG}(a1,b1) $. Alternatively, we can utilize the inverse gamma function from the scipy package, with the code being invgamma.rvs(a,b).\n",
        "\n"
      ],
      "metadata": {
        "id": "D0gn9iJBSXMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm, gamma, beta\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy.random import normal\n",
        "import pandas as pd\n",
        "import warnings\n",
        "def Kalman(R,H,A,Q_kal,true5D,Mu_st,N,P,K):\n",
        "# Function 1. Kalman filter (regime-switching)\n",
        "# Compared with common Kalman filter, there is one extra state-dependent term: Phi(L)*mu_st\n",
        "    k = 0\n",
        "    for t in range(N-1):\n",
        "        k += 1\n",
        "        x[:, k] = np.dot(A, x[:, k-1])+Mu_st[:,k]                                 # Kalman equation 1, Mu_st is the vector with state-dependent element\n",
        "        P = np.dot(A, np.dot(P, A.T)) +Q_kal                                      # Kalman equation 2\n",
        "        K = np.dot(P, np.dot(H.T, np.linalg.inv(np.dot(H, np.dot(P, H.T)) + R)))  # Kalman equation 3\n",
        "        x[:, k] = x[:, k] + np.dot(K, (true5D[:, k] - np.dot(H, x[:, k])))        # Kalman equation 4\n",
        "        P = np.dot((np.eye(P.shape[0]) - np.dot(K, H)), P)                        # Kalman equation 5\n",
        "        vct[0, k] = P[0, 0]  # save variance of delta_ct\n",
        "    return K, x, vct\n",
        "\n",
        "# Function 2. the process of adding t+1 data to the information set after running kalman filter\n",
        "# From this process, we will obtain mean & variance of delta c_{t|t+1}\n",
        "# The order of updating is backward, using data from T to update T-1 data,then update T-2...,3,2,1\n",
        "\n",
        "def afterkal(x,vct,Q_kal,phi,c):# add t+1 data to information set\n",
        "    for ii in range(N-2):       # this loop is just for xi_t and V_t conditional on delta_ct+1 for t=1,2,...,T-1\n",
        "        eta = c - phi[0]*x[0, N-ii-2] - phi[1]*x[0, N-ii-3] - phimu_st[N-ii-1]  # first input c is delta c_{T|T}\n",
        "                                                                                # start from 2nd iteration\n",
        "                                                                                # c is delta c_{t|t+1})\n",
        "        Rt = phi[0]**2*vct[0, N-ii-2]+phi[1]**2*vct[0,N-ii-3]+var_vt\n",
        "        xi[0, N-ii-2] = x[0, N-ii-2] + (phi[0]*vct[0, N-ii-2]+phi[1]*vct[0,N-ii-3])*eta/Rt\n",
        "        # we only use first row of the mean and variance vector\n",
        "        # so I calculated the first element of each mean & variance directly\n",
        "        V[0, N-ii-2] = vct[0, N-ii-2] - (phi[0]**2 * (vct[0,N-ii-2]**2)+phi[1]**2*(vct[0,N-ii-3]**2)) / Rt\n",
        "        c = normal(xi[0, N-ii-2], np.sqrt(V[0, N-ii-2]))\n",
        "    V[0, N-1] = vct[0, N-1] # V_T didn't get upgraded, it's still from step 1\n",
        "    xi[0, N-1] = x[0, N-1]  # xi_T didn't get upgraded, it's still from step 1\n",
        "    return xi,V\n",
        "\n",
        "# Function 3. calculating posterior mean & variance for normal distribution\n",
        "def posteriornormal(mean,var,a,b):\n",
        "\n",
        "    if np.isscalar(mean) and np.isscalar(var):\n",
        "        mean = 1/(1/var+a) * (mean/var + b )\n",
        "        var  = 1/(1/var+a)\n",
        "\n",
        "    else:\n",
        "        mean = np.linalg.inv(np.linalg.inv(var)+a) @ (np.linalg.inv(var) @ mean + b )\n",
        "        var  = np.linalg.inv(np.linalg.inv(var)+a)\n",
        "    if np.isnan(mean).any(): # if NaN shows up,reset to initial value\n",
        "        mean = np.zeros(mean.shape)\n",
        "        var = np.eye(var.shape[0])\n",
        "    return mean, var\n",
        "\n",
        "\n",
        "# Function 4. Generating random sample for posterior inverse gamma distribution\n",
        "def posteriorIG(a,b):\n",
        "    #input a,b here are posterior values\n",
        "    sig=1 / np.random.gamma(a, 1/b)\n",
        "    return(sig)"
      ],
      "metadata": {
        "id": "4v7m37561d27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################################\n",
        "#          pre-setting\n",
        "##########################################################################\n",
        "import numpy as np\n",
        "from scipy.stats import norm, gamma, beta\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy.random import normal\n",
        "import pandas as pd\n",
        "import warnings\n",
        "# Remove warning message of time series from each iteration\n",
        "def custom_warning_handler(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "warnings.showwarning = custom_warning_handler\n",
        "\n",
        "##########################################################################\n",
        "#          iteration setting\n",
        "##########################################################################\n",
        "iter = 10000 # number of iterations\n",
        "burn = 2000  # we burn the first 2000 iterations and sampling from the later 8000 iterations in the MCMC\n",
        "             # If we find the samples still can't converge after 2000 iterations,\n",
        "             # burn and whole iteration number can be modified.\n",
        "             # Another common choice is iteration 20000 times and burn the first 10000 iterations.\n",
        "\n",
        "             # A higher number of burns would make the results more stable & convincing\n",
        "             # but it would also make the program much slower\n",
        "\n",
        "##########################################################################\n",
        "#          input data\n",
        "##########################################################################\n",
        "delta_y_mean = np.zeros((5,1))\n",
        "for i in range(5):\n",
        "    delta_y_mean[i,0] = np.mean(kalpre1.iloc[:, i + 2]) # mean value of Y_it\n",
        "                                                        # This will be used in generating delta in appendix 7\n",
        "scale= StandardScaler()\n",
        "kalpre = scale.fit_transform(kalpre1)#standardize data\n",
        "\n",
        "np.random.seed(6)              # fix randomness\n",
        "N = kalpre.shape[0]            # length of input\n",
        "Y = np.transpose(kalpre[:,3:8])# Y_{1t} to Y_{5t}\n",
        "                               # Calculate delta_y, delta_y_t-1, and delta_y_t-2\n",
        "                               # Yit-E(Y), remove the mean value\n",
        "y = np.zeros((5, N))\n",
        "y1lag = np.zeros((5, N))\n",
        "y2lag = np.zeros((5, N))\n",
        "\n",
        "for i in range(5):\n",
        "    y[i, :] = kalpre[:, i + 2] - np.mean(kalpre[:, i + 2])\n",
        "    y1lag[i, :] = kalpre[:, i + 7] - np.mean(kalpre[:, i + 7])\n",
        "    y2lag[i, :] = kalpre[:, i + 12] - np.mean(kalpre[:, i + 12])\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# storage space for kalman filter output\n",
        "##########################################################################\n",
        "x = np.zeros((3, N))           # empty matrix to save mean value of xi_t\n",
        "xi=np.zeros((1,N))             # mean of delta c_{t|t+1}\n",
        "V=np.zeros((1,N))              # variance of delta c_{t|t+1}\n",
        "mu_st=np.zeros(N)              # regime-switching element mu_st\n",
        "phimu_st=np.zeros(N)           # regime-switching element with lags for kalman filter: phi(L)*mu_st,\n",
        "                               # phimu_st is input for Kalman filter equation 1\n",
        "new_delta_ct= np.zeros((N, 1)) # delta c_{t|t+1}\n",
        "\n",
        "\n",
        "# Form noise matrix Q_kal, correpodent with vector u^*_t=[var_vt 0 0 0 0 0]'\n",
        "var_vt=1                       # iid variance in equation (3), vt~iid N(0,1)\n",
        "                               # scale of new economic index is up to the choice of v_t's variance\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# Gibbs sampling\n",
        "##########################################################################\n",
        "Nn=kalpre.shape[0]-2            # length of input for Gibbs sampling\n",
        "rrighta3 = np.zeros((1, Nn - 2))# this matrix is for calculating distribution of lamda\n",
        "Q = np.zeros((Nn - 4, 2))\n",
        "Qstar = np.zeros((Nn - 4, 2))\n",
        "fvarphi = np.array([[1, 0], [0, 1]])\n",
        "mut = np.zeros(Nn)              #corresponding regime for t=3 to N, this is for estimating parameters\n",
        "meanlamda=np.zeros(5)\n",
        "varlamda=np.zeros(5)\n",
        "lamda=np.zeros(5)\n",
        "sig=np.zeros(5)\n",
        "\n",
        "# Initialize the DataFrame with the size of 8x419 with NaNs\n",
        "inputkal1 = np.full((Nn,8), np.nan)\n",
        "# Fill the columns 4 to 8 with the data from 'kalpre'\n",
        "inputkal1[:,3:8] = (y[:,2:N]).T # Nn = N-2\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# user-choose prior\n",
        "##########################################################################\n",
        "# variance and mean prior\n",
        "varlamdap=1\n",
        "meanlamdap=0\n",
        "varpsip = np.eye(2)\n",
        "meanpsip = np.zeros((2, 1))\n",
        "sig_a=(Nn-2)/2                 # when calculating sigma, length of applied series is Nn-2\n",
        "                               # sigma_1~IG(sig1a,sig1b) inverse gamma distribution\n",
        "                               # prior is IG(0,0),\n",
        "                               # because posterior is a fixed number : shape parameter prior+T/2 which is just T/2,\n",
        "                               # so we just set sig_i_a as (Nn-2)/2 here\n",
        "                               # T is the lenth of input series, here the length is Nn-2\n",
        "                               # if want different prior such as m, let sig1a=...=sig5a=m+T/2\n",
        "varmup=varphip = np.eye(2)\n",
        "meanmup=meanphip = np.zeros((2,1))\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# form storage space\n",
        "##########################################################################\n",
        "# We set some empty matrices here to save values from each iteration\n",
        "# After the 2000 burn-in period, we need to save value for later 80000 iterations\n",
        "vlamda=np.zeros((iter-burn,5))\n",
        "mlamda=np.zeros((iter-burn,5))\n",
        "bsig=np.zeros((iter-burn,5))\n",
        "vpsi=np.zeros((iter-burn,10))\n",
        "mpsi=np.zeros((iter-burn,10))\n",
        "mphi=np.zeros((iter-burn,2))\n",
        "vphi=np.zeros((iter-burn,2))\n",
        "mmu=np.zeros((iter-burn,2))\n",
        "vmu=np.zeros((iter-burn,2))\n",
        "mmu1 = np.zeros(iter-burn)\n",
        "mmu2 = np.zeros(iter-burn)\n",
        "vmu1 = np.zeros(iter-burn)\n",
        "vmu2 = np.zeros(iter-burn)\n",
        "delta=np.zeros(iter-burn)\n",
        "transition_prob_p=np.zeros(iter-burn)\n",
        "transition_prob_q=np.zeros(iter-burn)\n",
        "delta=np.zeros(iter-burn)\n",
        "regime=np.zeros((N-2, 1))\n",
        "e_it=np.zeros((5,N)) # for e_it i=1,2,3,4,5 (it's used when calculating delta)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# Initialization\n",
        "##########################################################################\n",
        "q = 0.9 # transition probability, recession to recession: q = P(S_t=0|S_t-1=0)\n",
        "p = 0.9 # transition probability, expansion to expansion: p = P(S_t=1|S_t-1=1)\n",
        "lamda=0.5*np.ones(5)\n",
        "sig=0.2*np.ones(5)\n",
        "psi=np.zeros((5,2))\n",
        "phi=np.zeros(2)\n",
        "mu0=-2\n",
        "mu1=2.5\n",
        "\n",
        "\n",
        "##################################  generate initial St based on transition probabilties p,q\n",
        "def generate_markov_chain(p, q, initial_state, N):\n",
        "    # Initialize the Markov Chain\n",
        "    markov_chain = np.zeros(N)\n",
        "    markov_chain[0] = initial_state\n",
        "\n",
        "    for i in range(1, N):\n",
        "        if markov_chain[i-1] == 1:\n",
        "            markov_chain[i] = np.random.choice([0, 1], p=[1-p, p])\n",
        "        else:\n",
        "            markov_chain[i] = np.random.choice([0, 1], p=[q, 1-q])\n",
        "\n",
        "    return markov_chain\n",
        "initial_state = 1  # initial state\n",
        "St = generate_markov_chain(p, q, initial_state, N)\n",
        "SSt=St\n",
        "mu_st = mu0 * np.ones(N) + mu1 * np.ones(N) * St\n",
        "phimu_st = (np.array(mu_st[2:N])\n",
        "            - np.array(mu_st[1:N-1]* phi[0])\n",
        "            - np.array(mu_st[0:N-2]* phi[1]))\n",
        "phimu_st = (np.array(mu_st[2:N])\n",
        "            - np.array(mu_st[1:N-1]* phi[0])\n",
        "            - np.array(mu_st[0:N-2]* phi[1]))\n",
        "phimu_st = np.array(phimu_st)\n",
        "phimu_st = np.insert(phimu_st, [0, 0], phimu_st[0])# As Phi(L)*mu_st has two lags\n",
        "                                                   # there will be 2 data loss at the beginning\n",
        "                                                   # we fill those two data points by 0\n",
        "                                                   # so for beginning two points of kalman filter\n",
        "                                                   # We run the Kalman filter as non-regime-switching Kalman filter\n",
        "                                                   # as regime-switching element Phi(L)*mu_st=0\n",
        "Mu_st=np.zeros((3,N))\n",
        "Mu_st[0,:]=phimu_st                                # form M^{~*}_{st}to fit the input size of kalman filter\n",
        "                                                   # other parts are just 0s\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# initial setting finishes here\n",
        "##############################################################################"
      ],
      "metadata": {
        "id": "THRIF9CKniRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Details of the algorithm**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In Gibbs sampling, we always use the most up-to-date information as input for each block of the Bayesian algorithm.\n",
        "\n",
        "There are 3 main blocks: Kalman filter, Hamilton (1989) basic filter and paraeter estimation through multimove Gibbs sampling. We will introduce detail of each block and how to iteratively connect each block.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "First step: Kalman filter\\\n",
        "input:\n",
        "1.  parameters generated from last step's multimove Gibbd sampling\n",
        "including $\\Phi(L), \\Psi_i(L), \\mu_0,\\mu_1,σ_i λ_i, i= 1,2,3,4,5$\n",
        "2.  series $Δ y_{it}, i=1,2,3,4,5$\n",
        "\n",
        "output: mean & variance of $Δ c_{t|t} , t=1, ⋯⋯, T-1, T$\n",
        "\n",
        "Then we make use of t+1 data of $Δ c_{t|t}$ to update $Δ c_{t|t}$ based on Kalman filter equations\n",
        "\n",
        "In the end of step 1: we get $Δ c_{t|t+1}$\n",
        "\n",
        "---\n",
        "\\begin{equation}\n",
        "\t\\Delta y^*_t=H^*\\zeta_t+\\epsilon_t\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\t\\zeta_t=\\tilde M^*_{s_t}+F^*\\zeta_{t-1}+u^*_t\n",
        "\\end{equation}\n",
        "which can be expanded as  \n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "\t\\Delta y^*_{1t}\\\\ \\Delta y^*_{2t}\\\\ \\Delta y^*_{3t}\\\\ \\Delta y^*_{4t}\\\\ \\Delta y^*_{5t}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\t\\lambda_1&\\ \\  -\\lambda_i\\psi_{11}&\\ \\ -\\lambda\\psi_{12}\\\\\n",
        "\t\\lambda_2&\\ \\ -\\lambda_i\\psi_{21}&\\ \\ -\\lambda\\psi_{22}\\\\\n",
        "\t\\lambda_3&\\ \\ -\\lambda_i\\psi_{31}&\\ \\ -\\lambda\\psi_{32}\\\\\n",
        "\t\\lambda_4&\\ \\ -\\lambda_i\\psi_{41}&\\ \\ -\\lambda\\psi_{42}\\\\\n",
        "\t\\lambda_5&\\ \\ -\\lambda_i\\psi_{51}&\\ \\ -\\lambda\\psi_{52}\\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\t\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2}\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "\t\\epsilon_{1t}\\\\\n",
        "\t\\epsilon_{2t}\\\\\n",
        "\t\\epsilon_{3t}\\\\\n",
        "\t\\epsilon_{4t}\\\\\n",
        "\t\\epsilon_{5t}\\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\t\\begin{bmatrix}\n",
        "\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2}\n",
        "\t\\end{bmatrix}\n",
        "\t=\n",
        "\t\\begin{bmatrix}\n",
        "\t\t\\Phi(L)\\mu_{s_t}\\\\\n",
        "\t\t0\\\\\n",
        "\t\t0\n",
        "\t\\end{bmatrix}\n",
        "\t+\\begin{bmatrix}\n",
        "\t\t\\phi_1&\\phi_2 & 0\\\\\n",
        "\t\t1&0&0\\\\\n",
        "\t\t0&1&0\n",
        "\t\\end{bmatrix}\n",
        "\t\\begin{bmatrix}\n",
        "\t\\Delta c_{t-1}\\\\ \\Delta c_{t-2} \\\\ \\Delta c_{t-3}\n",
        "\t\\end{bmatrix}\n",
        "\t+\n",
        "\t\\begin{bmatrix}\n",
        "\t\tv_t\\\\ 0 \\\\ 0\n",
        "\t\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Before starting Kalman filter, we form all the matrix above. Also, $Δ y^*_{it}=\\Psi_i(L)*Δ y_{it}$, we use new $\\Psi_i(L)$ estimated from last iteration to form new $Δ y^*_{it}$ at the beginning of Kalman filter in every iteration.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The function of Kalman filter is to estimate mean and variance of $\\zeta_t$, $\\zeta_t=\t\\begin{bmatrix}\n",
        "\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2}\n",
        "\t\\end{bmatrix} $.  \n",
        "\n",
        "Algorithm of Kalman filter is show as follows\n",
        "\n",
        "t iterates from t=2 to t=T\n",
        "\n",
        "For each iteration t, we use t-1 as input. $\\zeta_{t|t}$ from last iteration would become $\\zeta_{t-1|t-1}$ for next iteration. $P_{t|t}$ from last iteration would become $P_{t-1|t-1}$ for next iteration.\n",
        "\n",
        "1.\n",
        "\\begin{equation}\n",
        " \\zeta_{t|t-1}=F^*\\zeta_{t-1|t-1}+\\tilde M^*_{s_t}\n",
        " \\end{equation}\n",
        " Equation 1 is the process of **predicting** mean value of $\\zeta_t $ from $\\zeta_{t-1}$.  $\\  \\zeta_{t|t-1}=E[\\zeta_t|t-1]=E[F*\\zeta_{t-1}+\\phi(L)\\mu_{st}+v_t|t-1]=F*\\zeta_{t-1}$ as $\\mu_{st}$ is imposed mean zero in the setting according to Kim and Nelson (1998) and $v_t$ is iid noise of time t. For the first input $t=1$, we assume $\\zeta_{1|1}=0$. This is extended version of normal state-space Kalman filter as we added state-dependent element $M_{st}$ in the first kalman filter equation.\n",
        "   \n",
        "2.\n",
        " \\begin{equation}\n",
        " P_{t|t-1}=F^*P_{t-1|t-1}F^{*^T}+Q\n",
        " \\end{equation}\n",
        " Equation 2 is the process of **predicting** variance of $\\zeta_t $ from $\\zeta_{t-1}$. $Q$ is the variance matrix of process noise $[v_t,\\ v_{t-1},\\ 0]'$.  \n",
        "\n",
        " 3.\n",
        "\\begin{equation}\n",
        "\tK(t)= P_{t|t-1}H^{*^T}[H^*P_{t|t-1}H^{*^T}+R]^{-1}      \n",
        "\\end{equation}\n",
        "Equation 3 is the process of **filtering** or upgrading. Through this equation we can obtain the value of Kalman gain $K$. Kalman gain would stablize after a few iterations (which is called as steady-state Kalman gain). This is caused by the stablized variance matrix $P$ after a few iterations.\n",
        "  \n",
        "\n",
        "4.\n",
        "\\begin{equation}\n",
        " \\zeta_{t|t}= \\zeta_{t|t-1}+K(t)([\\Delta y^*_t-H^*\\zeta_{t|t-1}]\n",
        "\\end{equation}\n",
        "Equation 4 is the process of **filtering** mean value of $\\zeta_t$ by adding the filtered error term at time t.\n",
        "  \n",
        "\n",
        "5.\n",
        "\\begin{equation}\n",
        "\tP_{t|t}=[I-K(t)H^*]P_{t|t-1}\n",
        "\\end{equation}\n",
        "where $I$ is identity matrix.  \n",
        "Equation 5 is the process of **filtering** variance of $\\zeta_t$. $P$ would stablize with the stablization of Kalman gain after a few iterations.\n",
        "  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We iteratively run the algorithm from equation 1 to 5 from $t=2$ to $t=N$ and in the end we would obtain the filtered mean and variance of $Δ c_{t|t}$ for $t=1\\dots N$ from each iteration.\n",
        "\n",
        "After Kalman filter, we make full use of the information to improve the estimation result of Kalman filter.\n",
        "\n",
        " Take $\\Delta\\ c_{t+1}$ as observation and run a backward iteration to improve the effect of estimation.\n",
        "\n",
        "---\n",
        "\n",
        "We take $\\Delta c_{t+1}$ as extra observation and add  it to information set to lower the variance based on the following equation:\n",
        "\\begin{equation}\n",
        "\tp(\\tilde{\\zeta}_T|\\Delta\\tilde{y}^*_T)=p(\\zeta_T|\\Delta\\tilde{y}^*_T)\\mathop{\\Pi}\\limits^{T-1}_{t=1}p(\\zeta _t|\\Delta\\tilde{y}^*_t,\\zeta _{t+1})\n",
        "\\end{equation}\n",
        "where joint distribution $\\tilde{\\zeta}_T=[\\begin{matrix}\n",
        "\t\\zeta_1 &\\zeta_2& \\cdots &\\zeta_T\n",
        "\\end{matrix}]$' and $\\tilde{y}_T=[\\begin{matrix}\n",
        "\ty_1 &y_2& \\cdots &y_T\n",
        "\\end{matrix}]$'.\n",
        "\\\\ From last step, $\\zeta _{t|t}$ and $V_{t|t}$ is obtained and we generated $\\Delta c_T$. In this step, $\\Delta c_t$ for $t=1,2,\\cdots T-1$ is what we are going to generate. We follow \\citet{kim} and chapter 13 of \\citet{ham2}, the updating equations of adding $\\Delta c_{t+1}$ into information set are:  \n",
        "\n",
        "\\begin{equation}\n",
        "\t\\zeta_{t|t,\\Delta c_{t+1}}=\\zeta_{t|t}+V_{t|t}F^*(1)\\eta_t/R_t\n",
        "\\end{equation}  \n",
        "\n",
        "\\begin{equation}\n",
        "\tV_{t|t,\\Delta c_{t+1}}=V_{t|t}-V_{t|t}F^*(1)F^*(1)'V_{t|t}'/R_t\n",
        "\\end{equation}  \n",
        "\n",
        "where $\\eta_t=\\Delta c_{t+1}-\\Phi(L)\\mu_{t+1}-F^*(1)\\zeta_{t|t}$ and $E_t=F^*(1)V_{t|t}F^*(1)'+var[u^*_{t+1}(1)]$. $F^*(1)$ and $u^*_{t+1}(1)$ are first row of $F^*$ and $u^*_{t+1}$.\n",
        "  \n",
        "  With the new mean $\\zeta_{t|t,\\Delta c_{t+1}}$ and variance $V_{t|t,\\Delta c_{t+1}}$ conditional on $\\Delta c_{t+1}$, we can generate $\\Delta c_t$ for $t=1,2,\\cdots T-1$, we have also got $\\Delta c_T$ from last step. In the end, we get $\\Delta c_{t|t+1}$ for $t=1,2,\\cdots T$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        " **Second step: Hamilton (1989) basic filter**\n",
        "\n",
        "Here we applied MarkovAutoregression package in python. There is prepared Hamilton filter inside the package.\n",
        "We could also use Markovregression package which will be 10 times faster but we will miss many small recessions if we choose this package. It's fast but not informative.\n",
        "\n",
        "Input: series $\\Delta c_{t|t+1}$ from Kalman filter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Setting of Hamilton filter:\n",
        "1.  regime number:2\n",
        "2.  order of autoregression:4 (this could be different for different data) If we don't set order of autoregression properly, there could be two consequences: 1. improper etimation regime probabilities 2. the worst case is that hamilton filter would collapse (happened in actual tests). Hamilton filter can't generate regime probabilties (or just P($recession_t$) is a stochastic process around 0.5, meaningless estiamtion. For UK data, order=2 is the appropriate choice.\n",
        "\n",
        "\n",
        "Output:\n",
        "1. regime probabilities of recession $\\&$ expansion at each time t\n",
        "2. steady state transition probabilities between regimes\n",
        "3. regime series (if at time t, $P(recession)>P(expansion), S_t=0,$ otherwise $S_t=1$\n",
        "\n",
        "After Hamilton basic filter, there is one extra smooth process based on Bayes rule in Kim & Nelson (1998), this is also covered in the MarkovAutoregression package. We don't need to write extra code on this.\n",
        "\n",
        "Hamilton(1989) filter is critial part for this 'regime-switching' algorithm as it exposes the recession/expansion information from input data.\n",
        "\n",
        "One thing needs extra attention is that Hamilton(1989) basic filter can't handle extreme values. The program would collapse if extreme value shows up. There are a few things we can do if this happens.\n",
        "1.  Modify regime numbers. We can try giving extreme situation another indepednt regime. For example, current nodel here uses 2-regime setting, we could try 3 regimes.\n",
        "2.  Apply smoothers to the input data, actually kalman filter/ Kalman smoother has smoothed the data but in our algorithm, we applied the process of using t+1 data to improve estimation after running kalman filter. The good side is this will make input series more informative but the bad side is the new series generated after using t+1 data will break the smooth effect at the same time. So when you face the situation of extreme values, there are two options: smoothed but less informative data or informative data but there are extreme values.\n",
        "3.  Remove the process of using t+1 data after kalman filter. This process would make the input $\\Delta c_t$ more informative but this process can also lead to the existence of extreme values at the same time. Extreme value could cause the collapse of Hamilton (1989) basic filter. Point 2 $\\&$ 3 are talking about similar situation, the process of applying t+1 data is the source of this algorithm, we remove this process or we give extra smooth ( if the Hamilton filter collapses).\n",
        "4.  Romove the most unstable data period (such as 2020 recession in UK data test). Without this period, it's much easier for Hamilton basic filter to work. By the way, if we romove a period of extreme value, the generated recession probability series can be very different according to the actual tests. Very different probability series doesn't mean it's wrong. It's just because information we provided for this ML method gets very different, then the algorithm would tell us a new result based on this new series. The difference is: In former series, we can only see signals of very strong recession, probabilities of normal recession would only be about 0.1, 0.2, but we are expecting it to be over 0.5 as they are also real recessions. If we remove the most strong extreme values, the etimation for these common or weak recession would look better. Consequently, the effect of etimation normal time would be better and more reasonable. But we do care more about those extreme recessions rather than mild ones. For forecast or estimation of normal period, including extreme value would reduce the effictiveness but this may not be an appropriate choice for study of originally unstable stuff such as volatilities and study of exploring recession period/signals like what we are doing.\n",
        "5. We are also looking for a better algorithm that can fulfill the regime-switching related function and can handle extreme value at the same time (something better than Hamilton filter) but haven't find a good one.  \n",
        "\n",
        "Some extra setting after the Hamilton filter:\n",
        "\n",
        "As we set autoregression order=4, there are only N-4 data as output. The 4 missing data point at the beggining of series, we set them all as 1 according to kim's code. So the output St series are: [1,1,1,1, St]\\\n",
        "Would this affect estimation results? Very small influence. 1. For parameter estimation after hamilton filter, we use N-2 data, so only first 2 data are affected by our setting. 2. For Kalman filter, prediction and filtering part of Kalman filter is not accurate at the beginning part, the variance would stablize at around t=10. What's more, after running the code for the first time, we can modify the frist 4 St according to the estimation result of the model (such as [1 0 0 0 St] or [0 1 1 0 St], this would minimize the negative influence.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JMX139clc1pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Third step: parameter estimation**\n",
        "\n",
        "input: $Δ y_{it}$, $Δ c_t$, $S_t$, prior distribution of each parameter\n",
        "\n",
        "output: posterior distribution of each parameter.\n",
        "\n",
        "---\n",
        "Prior distributions for each parameter stay fixed during the process of MCMC.\n",
        "\n",
        "The equation of posterior distribution is calculated through MLE.\n",
        "\n",
        "In each calclulation of posterior distribution, we always use the most up-to-date copy of input.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Generating $\\lambda _i$, $i=1,2,3,4,5$\n",
        "Given $\\tilde{S}_T=\\begin{bmatrix}\n",
        "\tS_1& S_2 &\\cdots&S_T\n",
        "\\end{bmatrix}'$ from last step and $\\tilde{c}_T=\\begin{bmatrix}\n",
        "\tc_1& c_2 &\\cdots&c_T\n",
        "\\end{bmatrix}'$.\n",
        "\n",
        "\\begin{equation}\n",
        "\ty^*_{it}=\\lambda_i\\Delta c_t^*+\\epsilon _{it}\n",
        "\\end{equation}\n",
        "where $\\Delta c^*_t=\\Psi_i(L)\\Delta c_t$ and $\\Delta y^*_t=\\Psi_i(L)\\Delta y_t$. $Ψ_i(L)$ we use in one iteration is the most up-to-date sample generated from last iteration. Define $\\Delta \\tilde{C}^*$ and $\\Delta \\tilde{y}^*_i$ as variable vectors of right-hand-side and left-hand-side individually. We can then generate posterior distribution $\\lambda_i$ by:\n",
        "\n",
        "\n",
        "$$\n",
        "\\lambda_i \\sim N\\left(\\left(\\sigma_i^{-2} \\Delta\\tilde{C}^{\\prime} \\Delta\\tilde{C}^{\\ast} + X_i^{-1}\\right)^{-1}\\left(\\sigma_i^{-2} \\Delta\\tilde{C}^{\\prime} \\Delta\\tilde{y}_i^{\\ast} + X_i^{-1} x_i\\right), \\left(\\sigma_i^{-2} \\Delta\\tilde{C}^{\\prime} \\Delta\\tilde{C}^{\\ast} + X_i^{-1}\\right)^{-1}\\right)\n",
        "$$\n",
        "\n",
        "given prior distribution as\n",
        "\\begin{equation}\n",
        "\t\\lambda_i\\sim N(x_i,X_i)\n",
        "\\end{equation}\n",
        "\n",
        "$σ_i$ used in current iteration is sample generated from last itereation.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " Generating $\\psi$ and $\\sigma_i$, $i=1,2,3,4,5$\n",
        "Let $Z_t=y_{it}-\\lambda _i \\Delta c_t=e_{it}$ where $\\lambda _i$ is newly generated from last step.\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\Psi(L)Z_t=\\epsilon_{it}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\tZ_t=\\psi_{i1}Z_{t-1}+\\psi_{i2}Z_{t-2}+\\epsilon_{it}\n",
        "\\end{equation}\n",
        "\n",
        "Define $\\Delta \\tilde{Z}$ and $\\tilde{X}$ as variable vectors of right-hand-side and left-hand-side individually. We can then generate posterior distribution $\\tilde\\Psi_i$ ($\\tilde\\Psi_i=\\begin{bmatrix}\n",
        "\t\\psi_{i1}&\\psi_{i2}\n",
        "\\end{bmatrix}'$) by:\n",
        "$$\n",
        "\\tilde{\\Psi}_i \\sim N\\left(\\left(\\sigma_i^{-2} \\tilde{X}^{\\prime} \\tilde{X}^{\\ast} + \\Pi_i^{-1}\\right)^{-1}\\left(\\sigma_i^{-2} \\tilde{X}^{\\prime} \\tilde{Z}_i^{\\ast} + \\Pi_i^{-1} \\pi_i\\right),\\left(\\sigma_i^{-2} \\tilde{X}^{\\prime} \\tilde{X}^{\\ast} + \\Pi_i^{-1}\\right)^{-1}\\right)\n",
        "$$\n",
        "\n",
        "given prior distribution as\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\lambda_i\\sim N(\\pi _i, \\Pi _i)\n",
        "\\end{equation}\n",
        "With new $\\tilde\\Psi_i$ from above,\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\sigma^2_i\\sim IG(\\frac{a+T}{2},\\frac{b}{2}+\\frac{1}{2}(\\tilde Z-\\tilde X\\tilde \\Psi_i)'(\\tilde Z-\\tilde X\\tilde \\Psi_i))\n",
        "\\end{equation}\n",
        "\n",
        "given prior $$\\sigma^2 \\sim IG(\\frac{a}{2},\\frac{b}{2})$$\n",
        "where $IG$ means inverse gamma distribution. $a$ and $b$ are known value and we follow Kim & Nelson (1998) to set $a=b=0$.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " Generating $\\mu_0$, $\\mu_1$ and $\\tilde \\phi$\n",
        " To generate\n",
        "$\\tilde\\phi=\\begin{bmatrix}\\phi_1&\\phi_2\\end{bmatrix}'$, let $G_t=\\Delta c_t-\\mu_{s_t}$.\n",
        "\n",
        "\\begin{equation}\n",
        "\tG_t=\\phi_1G_{t-1}+\\phi_2G_{t-2}+v_t\n",
        "\\end{equation}\n",
        "let $\\tilde Q$ and $\\tilde G$ be right-hand-side and left-hand-side variable vector individually.\n",
        "We can generate posterior distribution of $\\Phi$.\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\tilde \\Phi\\sim N((\\tilde Q'\\tilde Q+X^{-1})^{-1}(\\tilde Q'\\tilde G+X^{-1}x),(\\tilde Q'\\tilde Q+X^{-1})^{-1})\n",
        "\\end{equation}\n",
        "given prior as\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\Phi\\sim N(x,X)\n",
        "\\end{equation}\n",
        "With new $\\Phi$ above, we let $G^*=\\Delta c_t-\\phi_1\\Delta c_{t-1}-\\phi_2\\Delta c_{t-2}$, then we can get\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\tG^*=\\mu_0^*+\\mu_1(S_t-\\phi_1S_{t-1}-\\phi_2S_{t-2})+v_t\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mu_0^*=\\mu_0(1-\\phi_1-\\phi_2)$. Let $\\tilde Q^*$ and $\\tilde G^*$ be right-hand-side and left-hand-side variable vector individually. Posterior distribution of $\\tilde \\mu=\\begin{bmatrix}\n",
        "\t\\mu_0^*&\\mu_t\n",
        "\\end{bmatrix}$ from.\n",
        "\n",
        "$$\n",
        "\\tilde{\\mu} \\sim N\\left(\\left(\\tilde{Q}^{\\ast\\prime}\\tilde{Q}^{\\ast} + X^{*^{-1}}\\right)^{-1}\\left(\\tilde{Q}^{\\ast\\prime}\\tilde{G}^{\\ast} + X^{*^{-1}}x^{\\ast}\\right), \\left(\\tilde{Q}^{\\ast\\prime}\\tilde{Q}^{\\ast} + X^{*^{-1}}\\right)^{-1}\\right)_{I(\\mu_1>0)}\n",
        "$$\n",
        "\n",
        "given prior as\n",
        "\\begin{equation}\n",
        "\t\\mu\\sim N(x^*,X^*)\n",
        "\\end{equation}\n",
        "\n",
        "Attention:\n",
        "1. $\\mu_1$ has to be greater than 0, as state 1 represents expansion regime, there should be an positive deviation from the mean growth rate of economic index. if $ \\mu_1<0 $, redo the sampling again until we generate a posive $\\mu_1$ from $N(mean_{\\mu_1}, var_ {\\mu_1})$.\n",
        "\n",
        "2.  The genrated mean & variance of $\\mu_0^*$ is the mean and variance of $\\mu_0*(1-\\phi_1-\\phi_2)$. In order to generate $\\mu_0$ we generate a sample from the distribution of $\\mu_0^*$, then we divided the sample $\\mu_0=\\frac{\\mu_0^*} {1-\\phi_1-\\phi_2}$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "Generate $\\delta$\n",
        "\n",
        "Based on  equation 10 and 11 in K$\\&$N(1998), we run the Kalman filter again to generate steady-state Kalman gain $K^*$, then we use the steady-state Kalman gain to generate $\\delta$: \\begin{equation*}\n",
        "\\delta=E_1'[I_k-(I_k-K^*H)F]^{-1}K^*\\Delta \\overline{Y}\n",
        "\\end{equation*}\n",
        "(There was a typo in the equation in Kim's paper, The first $K_k$ in paper (in equation A.27) is actually $I_k$ according to Kim's GAUSS code and original calculation in Stock and Watson (1991).\n",
        "k is the dimension of F,  $\\ \\Delta \\overline{Y}=\\begin{bmatrix} \\Delta \\overline{Y_1}&\\Delta \\overline{Y_2}&\\Delta \\overline{Y_3}&\\Delta \\overline{Y_4}&\\Delta \\overline{Y_5}\\end{bmatrix}' $.\n",
        "Difference with former Kalman filter above: here we used $\\Delta y_i=λ(L)*\\Delta c_t+e_{it}$\n",
        "rather than $\\Psi(L)\\Delta y_i=λ(L)\\Psi(L)\\Delta c_t+\\epsilon_{it}$ and $e_{it}$ would also need to be estimated because $e_{it}$ is not a iid noise, we can't treat it like one and put it into the covariance matrix. So in actual programming, the covariance matrix R (which is correpodent with $\\epsilon_t$) will be a $4*4$ zero-matrix (it was a $4*4$ diagonal matrix with $\\sigma_i^2$ at the diagonals in former Kalman filter).\n",
        "\\begin{equation}\n",
        "\t\\Delta y^*_t=H^*\\zeta_t\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\t\\zeta_t=\\tilde M^*_{s_t}+F^*\\zeta_{t-1}+u^*_t\n",
        "\\end{equation}\n",
        "\n",
        "Where\n",
        "\n",
        "$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "  \\phi_1 & \\phi_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 0 & 0 & \\psi_{11} & \\psi_{12} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 0 & 0 & 0 & 0 & \\psi_{21} & \\psi_{22} & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\psi_{31} & \\psi_{32}& 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "\t0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\psi_{41} & \\psi_{42}  & 0 & 0 \\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0  \\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\psi_{51} & \\psi_{52} \\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "$H =\n",
        "\\begin{bmatrix}\n",
        "\\lambda _1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "\\lambda _2 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "\\lambda _3 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "\\lambda _4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
        "\\lambda _5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "$Q =\n",
        "\\begin{bmatrix}\n",
        "var(v_t) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & \\sigma_1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & \\sigma_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_3 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_4 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_5 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "\\end{bmatrix}\n",
        "$\n",
        "$\n",
        "\\zeta_t=\n",
        "\\begin{bmatrix}\n",
        "Δ c_t\\\\\n",
        "Δ c_{t-1}\\\\\n",
        "Δ c_{t-2}\\\\\n",
        "e_{1,t}\\\\\n",
        "e_{1,t-1}\\\\\n",
        "e_{2,t}\\\\\n",
        "e_{2,t-1}\\\\\n",
        "e_{3,t}\\\\\n",
        "e_{3,t-1}\\\\\n",
        "e_{4,t}\\\\\n",
        "e_{4,t-1}\\\\\n",
        "e_{5,t}\\\\\n",
        "e_{5,t-1}\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "By running Kalman filter for this setting, we want to calculate only one value: steady-state Kalman gain. Kalman gain is generated from Kalman filter equation 3 ($K (t)= P_{t|t-1}H^{*^T}[H^*P_{t|t-1}H^{*^T}+R]^{-1}   $) and value of K will stablize after a few iterations. So if we want to gain steady-state Kalman Gain, we can directly take the Kalman gain from the final iteration.\n",
        "\n",
        "---\n",
        "Generate new economic index $C_t$\n",
        "\n",
        "After obtaining $\\delta$ and $\\Delta c_t$, We can generate the final economic index by following the equation $C_t=\\Delta c_t+C_{t-1}+\\delta$, $t=1,2,\\cdots , T$. We need value $C_0$ to start iteratively generating $C_t$, but the chocie of $C_0$ can be arbitrary. We choose $C_0=99$ here as this will make it easier to have some comparison with original economic index.\n",
        "\n",
        "Another thing needs extra comment is: the trend of new economic index\n",
        "\n",
        "The trend purely comes from the element $\\delta$, we add one more $\\delta$ every single time in $C_t=\\Delta c_t+C_{t-1}+\\delta$. But scale of $\\delta$ depends on the mean value of $Y_{it}$, $\\delta$ could be very small and there will be almost no trend in the new economic index ( this happens for the UK data result)."
      ],
      "metadata": {
        "id": "xV9z_mZgmRhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##############################################################################\n",
        "#Start iteration here\n",
        "##############################################################################\n",
        "\n",
        "for i in range(iter):\n",
        "###########################################################################\n",
        "########## First part of MCMC------- Kalman Filter\n",
        "###########################################################################\n",
        "\n",
        "# Use the parameters from last iteration\n",
        "# we need new A,R,H\n",
        "\n",
        "\n",
        "    R = np.diag(sig)\n",
        "    H = np.zeros((5,3))\n",
        "    for m in range(5):\n",
        "      H[m,0]=lamda[m]\n",
        "      H[m,1]=-lamda[m]*psi[m,0]\n",
        "      H[m,2]=-lamda[m]*psi[m,1]\n",
        "    H = np.squeeze(H)\n",
        "    A = np.array([[phi[0], phi[1], 0],\n",
        "              [1, 0, 0],\n",
        "              [0, 1, 0]])\n",
        "    Q_kal = np.zeros((3,3))\n",
        "    Q_kal[0,0]=var_vt\n",
        "    P = 0.1 * np.eye(3)# initial setting of variance matrix of xi_t\n",
        "    # z = np.ones((5, N))\n",
        "    x = np.zeros((3, N))   # empty matrix to save mean value of xi_t\n",
        "    vct = np.zeros((3, N)) # empty matrix to save variance of xi_t\n",
        "    K=np.zeros(H.T.shape)\n",
        "# Construct delta_y* using new psi\n",
        "    ystar = y.T - psi[:, 0]* y1lag[0:5, :].T- psi[:, 1]*y2lag[0:5, :].T\n",
        "    true5D = ystar.T\n",
        "    Mu_st=np.zeros((3,N))\n",
        "    Mu_st[0,:]=phimu_st\n",
        "    K,x,vct=Kalman(R,H,A,Q_kal,true5D,Mu_st,N,P,K)\n",
        "# output would be mean & variance of delta c_t, t=1...T\n",
        "\n",
        "    c = normal(x[0, N-1], np.sqrt(vct[0, N-1]))  # generate delta_c_T\n",
        "    # add t+1 data to information set\n",
        "    xi, V= afterkal(x,vct,Q_kal,phi,c)\n",
        "\n",
        "########################\n",
        "# generate new delta_ct based on new mean and variance\n",
        "\n",
        "    for iii in range(0, N):\n",
        "        new_delta_ct[iii] = np.mean(normal(xi[0,iii], np.sqrt(V[0,iii]),1000)) # new delta_c_t\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "############# Second step of MCMC-------- Hamilton (1989) basic filter\n",
        "###########################################################################\n",
        "\n",
        "# Use output from Kalman filter to generate regime probability then generate St\n",
        "    data= new_delta_ct.flatten()\n",
        "# Create date range\n",
        "    date_range = pd.date_range(start='1986-02', end='2021-03', freq='M')\n",
        "# Create DataFrame with date range and data\n",
        "    df = pd.DataFrame({'date': date_range, 'delta_ct': data})\n",
        "    df.set_index('date', inplace=True)\n",
        "\n",
        "# Fit the model\n",
        "    mod = sm.tsa.MarkovAutoregression(\n",
        "        df['delta_ct'], k_regimes=2, trend=\"n\", order=2, switching_variance=True\n",
        ")\n",
        "    res = mod.fit()\n",
        "\n",
        "#res_hamilton.filtered_marginal_probabilities\n",
        "# standard is not fixed, for some US recession signal, standard is >0.4\n",
        "    St = (res.smoothed_marginal_probabilities[1] < 0.5).astype(int)\n",
        "    SSt = St.reset_index(drop=True)\n",
        "    # 2 data loss at the beginning of series as we set autoregression order=4\n",
        "    # we set 2 missing points as 1,1 (expansion), details can be checked from the notes\n",
        "    SSt = np.concatenate(([1,1], SSt))#new St\n",
        "    St=SSt[2:N].reshape(-1,1)\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "############# Third step of MCMC-------- Gibbs sampling\n",
        "###########################################################################\n",
        "\n",
        "# Input: St & inputkal1 (delta c_t and delta y_it)\n",
        "# St is ready, we now prepare inputkal1\n",
        "\n",
        "# Fill the first three columns with t, t-1, t-2 data from 'new_delta_ct' from each iteration\n",
        "    for iiii in range(3):\n",
        "        inputkal1[:, iiii] = new_delta_ct[2 - iiii:N - iiii].flatten()\n",
        "# data length for code below is Nn, (Nn=N-2)\n",
        "\n",
        "#There are 3 sections of Gibbs sampling, correpsonding notes can be found above\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "############# Section 1-------- generate new lamda\n",
        "###########################################################################\n",
        "# posterior distribution of lamda\n",
        "\n",
        "    righta3 = inputkal1[:, 0].T\n",
        "    for m in range(5):\n",
        "      ys=(inputkal1[2:Nn, 3+m] - inputkal1[1:Nn-1, 3+m] * psi[m, 0] - inputkal1[0:Nn-2, 3+m] * psi[m,1]).reshape(-1, 1)\n",
        "      rrighta=(righta3[2:Nn] - righta3[1:Nn-1] * psi[m, 0] - righta3[0:Nn-2] * psi[m,1]).reshape(-1, 1)\n",
        "      meanlamda[m], varlamda[m]= posteriornormal(meanlamdap,varlamdap, rrighta.T@ rrighta/sig[m], rrighta.T@ys/sig[m])\n",
        "      lamda[m]=np.random.normal(meanlamda[m], np.sqrt(varlamda[m]))\n",
        "\n",
        "###########################################################################\n",
        "############# Section 2 -------- generate new Psi_i & sigma_i^2\n",
        "###########################################################################\n",
        "# Form Z and X then generate posterior psi\n",
        "    for m in range(5):\n",
        "      Zpsi=inputkal1[:, 3+m] - inputkal1[:, 0] * lamda[m]\n",
        "      Xpsi=np.column_stack((Zpsi[1:Nn - 1], Zpsi[:Nn - 2]))\n",
        "      meanpsi, varpsi = posteriornormal(meanpsip,varpsip,Xpsi.T @ Xpsi / sig[m],(Xpsi.T @ Zpsi[2:Nn]).reshape(-1, 1) / sig[m])\n",
        "      psi[m,:]=np.random.multivariate_normal(meanpsi.flatten(), varpsi)\n",
        "# Now we start to generate sigma_i\n",
        "      Xx=Xpsi*psi[m,:]\n",
        "      sig_b=0.5*np.dot(((Zpsi[2:Nn] - Xx[:, 0] - Xx[:, 1]).reshape(-1, 1)).T, (Zpsi[2:Nn] - Xx[:, 0] - Xx[:, 1]).reshape(-1, 1))\n",
        "      sig[m]=posteriorIG(sig_a, sig_b)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "############# Section 3 -------- generate new phi and mu\n",
        "###########################################################################\n",
        "\n",
        "# Form G and Q as shown in the overview of section 3\n",
        "    G = inputkal1[4:Nn, 0] - mut[4:Nn]\n",
        "    Q[:, 0] = inputkal1[3:Nn-1, 0] - mut[3:Nn-1]\n",
        "    Q[:, 1] = inputkal1[2:Nn-2, 1] - mut[2:Nn-2]\n",
        "    meanphi, varphi =posteriornormal(meanphip,varphip,Q.T @ Q,(Q.T @ G).reshape(-1, 1))\n",
        "\n",
        "# sampling new phi\n",
        "    phi=np.random.multivariate_normal(meanphi.flatten(), varphi)#new phi generated from last iteration\n",
        "    counter=0\n",
        "    while abs(1-sum(phi))<0.1 and counter < 100:\n",
        "      phi=np.random.multivariate_normal(meanphi.flatten(), varphi)\n",
        "\n",
        "\n",
        "      counter += 1\n",
        "\n",
        "# generating mu\n",
        "# form G* and Q* as shown in the overview of section 3\n",
        "    Gstar = (inputkal1[4:Nn, 0] - phi[0]* inputkal1[3:Nn - 1, 0]- phi[1] * inputkal1[2:Nn - 2, 0]).reshape(-1, 1)\n",
        "    Qstar = np.zeros((Nn - 4, 2))\n",
        "    Qstar[:, 0] = np.ones(Nn-4)\n",
        "    Qstar[:, 1] = (St[4:Nn].flatten()\n",
        "                   - phi[0]* St[3:Nn - 1].flatten()\n",
        "                   - phi[1] * St[2:Nn - 2].flatten()).reshape(-1,)\n",
        "\n",
        "# generate posterior variance of mu\n",
        "    meanmu, varmu = posteriornormal(meanmup,varmup, Qstar.T @ Qstar,Qstar.T @ Gstar)\n",
        "# attention, this distribution for mu_o* and mu_1, so we need to calculate mu_0\n",
        "# mu_0*=mu_0(1-phi1-phi2)\n",
        "\n",
        "    mu=np.random.multivariate_normal(meanmu.flatten(), varmu)\n",
        "    mu0=(mu[0]/(1 -np.sum(phi)))\n",
        "    mu1=mu[1]\n",
        "#    mu0=(np.random.normal(meanmu[0], np.sqrt(varmu[0, 0]))/(1 -np.sum(phi))).item()\n",
        "#    mu1=(np.random.normal(meanmu[1], np.sqrt(varmu[1, 1]))).item()\n",
        "\n",
        "    counter=0\n",
        "    while mu1 <= 0 and counter < 100:\n",
        "      mu=np.random.multivariate_normal(meanmu.flatten(), varmu)\n",
        "      mu0=(mu[0]/(1 -np.sum(phi)))\n",
        "      mu1=mu[1]\n",
        "\n",
        "      counter += 1\n",
        "    if mu1 <= 0:\n",
        "      mu1 = 0.5\n",
        "\n",
        "    mut =( mu0 * np.ones(Nn)\n",
        "        + mu1 * np.ones(Nn) * St.flatten().flatten()\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# main part of gibbs sampling iteration finishes here\n",
        "########################################################################\n",
        "\n",
        "\n",
        "########################################################################\n",
        "########################################################################\n",
        "# generate delta\n",
        "# run kalman filter based on equation 10 and 11 in K&N (1998) to calculate delta\n",
        "# replace by a kalman filter function funstion\n",
        "\n",
        "    A = np.block(\n",
        "             [[phi[0], phi[1], 0, *np.zeros(10)],\n",
        "              [1,           0, 0, *np.zeros(10)],\n",
        "              [0,           1, 0, *np.zeros(10)],\n",
        "              [*np.zeros(3), psi[0,0], psi[0,1], *np.zeros(8)],\n",
        "              [*np.zeros(3), 1, 0, *np.zeros(8)],\n",
        "              [*np.zeros(5), psi[1,0], psi[1,1], *np.zeros(6)],\n",
        "              [*np.zeros(5), 1, 0, *np.zeros(6)],\n",
        "              [*np.zeros(7), psi[2,0], psi[2,1], *np.zeros(4)],\n",
        "              [*np.zeros(7), 1, 0, *np.zeros(4)],\n",
        "              [*np.zeros(9), psi[3,0], psi[3,1], *np.zeros(2)],\n",
        "              [*np.zeros(9), 1, 0, *np.zeros(2)],\n",
        "              [*np.zeros(11), psi[4,0], psi[4,1] ],\n",
        "              [*np.zeros(11), 1, 0]\n",
        "              ])\n",
        "\n",
        "    H = np.block([[lamda[0], *np.zeros(2),  1, *np.zeros(9)],\n",
        "                  [lamda[1], *np.zeros(4),  1, *np.zeros(7)],\n",
        "                  [lamda[2], *np.zeros(6),  1, *np.zeros(5)],\n",
        "                  [lamda[3], *np.zeros(8),  1, *np.zeros(3)],\n",
        "                  [lamda[4], *np.zeros(10), 1, *np.zeros(1)]\n",
        "                  ])\n",
        "\n",
        "    H = np.squeeze(H)\n",
        "    Qdelta = np.zeros((13, 13))\n",
        "    Qdelta[0, 0] = var_vt\n",
        "    for m in range (5):\n",
        "      Qdelta[3+2*m, 3+2*m] = sig[m]\n",
        "\n",
        "    P = 0.1 * np.eye(A.shape[0])# initial setting of variance matrix of xi_t\n",
        "    z = np.ones((5, N))\n",
        "    true5D = Y\n",
        "    K=np.zeros(H.T.shape)\n",
        "#form new mu_st\n",
        "    St=SSt\n",
        "    mu_st = (mu0 * np.ones(N) + mu1 * np.ones(N) * St\n",
        "    )\n",
        "    phimu_st=np.zeros(N)\n",
        "\n",
        "    phimu_st = (np.array(mu_st[2:N])\n",
        "            - np.array(mu_st[1:N-1]* phi[0])\n",
        "            - np.array(mu_st[0:N-2]* phi[1]))\n",
        "    phimu_st = np.array(phimu_st)\n",
        "    phimu_st = np.insert(phimu_st, [0, 0], phimu_st[0])\n",
        "    phimu_st = np.where(np.abs(phimu_st) > 10, 0, phimu_st)\n",
        "    Mu_st=np.zeros((13,N))\n",
        "    Mu_st[0,:]=phimu_st\n",
        "    R = np.zeros((5,5))\n",
        "    x = np.zeros((13, N))#empty matrix to save mean value of xi_t\n",
        "    vct = np.zeros((13, N))#empty matrix to save variance of xi_t\n",
        "    K,x,vct=Kalman(R,H,A,Qdelta,true5D,Mu_st,N,P,K)\n",
        "    newdelta=(np.linalg.inv(np.eye(P.shape[0])-(np.eye(P.shape[0]) - K @ H)@A) @ K  @ delta_y_mean)[0,0]\n",
        "\n",
        "#########################################################################\n",
        "# same value of each iteration after first 2000 iterations\n",
        "    if i>burn-1:\n",
        "\n",
        "#########################################################################\n",
        "\n",
        "#      p_00 = 1-res.params[1]   #### this is q: recession to recession P(S_t=0|S_t-1=0)\n",
        "#      p_11 = res.params[0]  #### this is p: expansion to expansion P(S_t=1|S_t-1=1)\n",
        "      for m in range(5):\n",
        "        mlamda[i-burn,m]=lamda[m]\n",
        "        bsig[i-burn,m]=sig[m]\n",
        "        mpsi[i-burn,2*m]=psi[m,0]\n",
        "        mpsi[i-burn,2*m+1]=psi[m,1]\n",
        "      mphi[i-burn,:] = phi\n",
        "      mmu1[i - burn] = mu0\n",
        "      mmu2[i - burn] = mu1\n",
        "      transition_prob_p[i-burn]=res.params[0]  #### this is p: expansion to expansion P(S_t=1|S_t-1=1)\n",
        "      transition_prob_q[i-burn]=1-res.params[1]    #### this is q: recession to recession P(S_t=0|S_t-1=0)\n",
        "      delta[i-burn]=newdelta\n",
        "      sample_ct=sample_ct+new_delta_ct\n",
        "      regime=regime+res.smoothed_marginal_probabilities[1].values.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "Y6tGP_t-xGsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final output of parameter distributions**"
      ],
      "metadata": {
        "id": "iz-mG5bnd-XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_stat(label, array):\n",
        "    mean_val = np.mean(array)\n",
        "    std_val = np.sqrt(np.var(array))\n",
        "    median_val = np.median(array)\n",
        "    low_perc = np.percentile(array, 2.5)\n",
        "    high_perc = np.percentile(array, 97.5)\n",
        "\n",
        "    print(f\"{label}: {mean_val:.4f} {std_val:.4f} {median_val:.4f} ({low_perc:.4f}, {high_perc:.4f})\")\n",
        "\n",
        "sample_ct_mean = np.mean(sample_ct)\n",
        "recession = regime/(iter-burn)\n",
        "\n",
        "print(\"        Mean     \", \"SD\", \"    Median \", \"  95% band\")\n",
        "print(\"delta c_t\")\n",
        "\n",
        "print_stat(\"phi\", mphi)\n",
        "print_stat(\"mu1\", mmu1)\n",
        "print_stat(\"mu2\", mmu2)\n",
        "print_stat(\"p\", transition_prob_p)\n",
        "print_stat(\"q\", transition_prob_q)\n",
        "print_stat(\"delta\", delta)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Y_{i}\")\n",
        "    print_stat(f\"lamda_{i+1}\", mlamda[:,i])\n",
        "    print_stat(f\"sigma^2_{i+1}\", bsig[:,i])\n",
        "    print_stat(f\"psi_{i+1}1\", mpsi[:,2*i])\n",
        "    print_stat(f\"psi_{i+1}2\", mpsi[:,2*i+1])\n"
      ],
      "metadata": {
        "id": "TTwWC8_AnwR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "####### plot of recession probability\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Create a new DataFrame to store the ratios\n",
        "date_range_ratios = pd.date_range(start='1986-02', end='2021-03', freq='M')\n",
        "df_ratios = pd.DataFrame({'date': date_range_ratios, 'ratio': recession.ravel()})\n",
        "df_ratios.set_index('date', inplace=True)\n",
        "\n",
        "# Plot the ratio series\n",
        "df_ratios['ratio'].plot(title=\"Recession\", figsize=(20, 3))\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HWdqfA2Knz-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import normal\n",
        "N=kalpre.shape[0]\n",
        "ddelta=np.mean(delta)\n",
        "C0=99.72203\n",
        "newCLI = np.zeros((N, 1))\n",
        "newCLI[0]=C0+(sample_ct/(iter-burn))[0]+ddelta\n",
        "for i in range(1, N):\n",
        "    newCLI[i] = newCLI[i-1] +  (sample_ct/(iter-burn))[i]+ddelta # new delta_C_t=delta_c_t+C_{t-1}+delta according to appendix A7\n",
        "    # Create an array of monthly dates\n",
        "dates = pd.date_range(start='1986-03-01', end='2021-3-01', freq='MS')\n",
        "\n",
        "# Create a pandas Series with the data and the dates as index\n",
        "series1 = pd.Series(data=newCLI.ravel(), index=dates)\n",
        "fig, ax = plt.subplots(figsize=(10,3))\n",
        "# Plot the series\n",
        "series1.plot(figsize=(20,3), legend=False,label='new economic index with trend',color='orange')\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig('myplot.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-klEyqJDn2jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "url = 'https://raw.githubusercontent.com/Buchunwang/UK-CLI/main/Kalman%20filter/CLI_raw.csv'#original CLI data\n",
        "CLI = pd.read_csv(url, header=None, encoding='utf-8', skiprows=1)\n",
        "# Create an array of monthly dates\n",
        "dates = pd.date_range(start='1986-03-01', end='2021-3-01', freq='MS')\n",
        "\n",
        "# Create a pandas Series with the data and the dates as index\n",
        "series1 = pd.Series(data=newCLI1.ravel(), index=dates)\n",
        "fig, ax = plt.subplots(figsize=(10,3))\n",
        "# Plot the series\n",
        "series1.plot(figsize=(10,3), legend=False,label='New UK economy index without trend',color='orange')\n",
        "\n",
        "# Create another pandas Series with the same dates as index\n",
        "series2 = pd.Series(CLI.values.ravel(), index=dates)\n",
        "# Plot the series\n",
        "\n",
        "# Plot the second series\n",
        "series2.plot(figsize=(10,3), legend=False,label='CLI from OECD')\n",
        "\n",
        "\n",
        "\n",
        "# Set the y-axis limits\n",
        "#ax.set_ylim(80, 110)\n",
        "plt.legend()\n",
        "plt.savefig('myplot.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sHqlE7q9n5Up"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}