{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM78bpyjFqzJjSAc5zBVN4n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Buchunwang/UK-CLI/blob/main/K%26N_data_code_with_new_notes_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Replication of Kim \\& Nelson (1998)**\n",
        "\n",
        "---\n",
        "**Brief Introduction of the Code**\n",
        "\n",
        "This is a Bayesian algorithm with three main blocks:\n",
        "\n",
        "\n",
        "1. Kalman filter\n",
        "2. Hamilton(1989) basic filter\n",
        "3. Estiamting parameters by multi-move Gibbs sampling.\n",
        "\n",
        "We iteratively run the process in the sequence 1$\\to$2$\\to$3$\\to$1 $\\cdots$.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        " From the algorithm, we can generate:\n",
        "1. a new economic index of US\n",
        "2. historical recession period of US\n",
        "3. In the end, we will also display the distribution of all the parameters.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**Data**\n",
        "\n",
        "For columns 1:4, they represent component variables $\\Delta y_i, \\ i=1,2,3,4$ used to generate the original coincident index. Columns 5:8 and 9:12 correspond to component variables with 1 lag and 2 lags, respectively: $\\Delta y_{i,t-1}$ and $\\Delta y_{i,t-2}, \\ i=1,2,3,4$.\n",
        "\n",
        "All imported data have been detrended. The detrending process is:\n",
        "1. we have 4 series $Y_{i,t}$, $i=1,2,3,4$ which are\n",
        "   1.  series 1: industrial production (IP),\n",
        "   2.  series 2: total personal income less transfer payments in 1987 dollars (GMYXPQ),\n",
        "   3.  series 3: total manufacturing and trade sales in 1987 dollars (MTQ),\n",
        "   4.  series 4: employees on nonagricultural payrolls (LPNAG ).\n",
        "2. take log of each series: $ln(Y_{i,t})$\n",
        "3. multiply 100, we get: $100*ln(Y_{i,t})$\n",
        "4. finally, we generate the detrended $y_{it}$ by taking first difference: $\\Delta Y_{i,t}=100*ln(Y_{i,t})-100*ln(Y_{i,t-1})$\n",
        "\n",
        "\\noindent The data range selected is from 09/1959 to 01/1995. The final outputs of the entire MCMC algorithm ($S_t$ and $C_t$) span from 01/1960 to 01/1995. However, due to the loss of 4 data points during calculations (attributable to autoregression and lags), we've included an additional 4 data points (from 09/1959 to 12/1959) prior to the start point of 01/1960 to compensate.\n",
        "\n",
        "Data source: The Conference Board.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EpbbzJMsM133"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import data\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "url = 'https://raw.githubusercontent.com/Buchunwang/UK-CLI/main/Kalman%20filter/kndata4.csv'\n",
        "print('From URL:', url)\n",
        "kalpre1 = pd.read_csv(url, header=None, encoding='utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8UU-Iq2mYozo",
        "outputId": "ff55ff4b-2a0c-4346-b4bc-d81ad72e8a3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From URL: https://raw.githubusercontent.com/Buchunwang/UK-CLI/main/Kalman%20filter/kndata4.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Modeling of the algorithm**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.    \\begin{equation} \\Delta Y_{it}=\\lambda_i(L)\\Delta C_t+D_i+e_{it} \\ \\ \\ i=1,2,3,4\\ \\ \\ t=1,2… T \\end{equation} $Y_{it}$ are component series above except passenger car registrations, $C_t$ is the growth rate of CLI,$\\lambda _i(L)=\\lambda _i $ for $i=1,2,3$, $\\lambda _i(L)=\\lambda _{i0}+\\lambda _{i1}*L+\\lambda _{i2}*L^2+\\lambda _{i3}*L^3$ for $i=4$.\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "2.  \\begin{equation} \\Psi_i(L)e_{it}=\\epsilon_{it},  \\ \\ \\ \\epsilon_{it}\\sim iid N(0,\\sigma_i^2)\t  \\end{equation} where $\\Psi_i(L)=1-\\psi_{i1}L-\\psi_{i2}L^2$, $L$ is the lag operator\n",
        "\n",
        "---\n",
        "\n",
        "3.  \\begin{equation}\\Phi(L)(\\Delta C_t-\\mu_{s_t}-\\delta)=v_t\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ v_t\\sim iid N(0,1)\\end{equation} where $\\Phi(L)=1-\\phi_1L-\\phi_2L^2$. $\\mu_{s_t}-\\delta$ is the mean growth rate of $\\Delta C_t$, $\\mu_{s_t}$ is the regime-switching component.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "4.  \\begin{equation}\n",
        "\t\\mu_{s_t}=\\mu_0+\\mu_1S_t\n",
        "    \\end{equation}\n",
        "    where $S_t=\\{\\begin{matrix} 0,1 \\end{matrix}\\}$ and $\\mu_1<0$.\n",
        "    The regime-switching happens at the mean value of $\\Delta C_t$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We remove the mean value of $\\Delta Y_{it}$ and $\\Delta C_t$:\n",
        "\\begin{equation}\n",
        "\\Delta c_t=\\Delta C_t-\\delta\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\Delta y_{it}=\\Delta Y_{it}-\\Delta \\overline{Y_i}\n",
        "\\end{equation}\n",
        "\n",
        "---\n",
        "\n",
        "Equation 1 and 3 can then be replaced by\n",
        "\n",
        "5.\n",
        "\\begin{equation}\n",
        "\\Delta y_{it}=\\lambda_i(L)\\Delta c_t+e_{it} \\ \\ \\ i=1,2,3,4 \\ \\ \\ t=1,2… T\n",
        "\\end{equation}\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "6.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\Phi(L)(\\Delta c_t-\\mu_{s_t}\n",
        ")=v_t\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ v_t\\sim iid N(0,1)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "JpuRlORwM0Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are 4 functions we are going to use:\n",
        "1.  Kalman Filter (regime-switching state-space Kalman filter)\n",
        "2.  Update process after Kalman filter using t+1 data for time t\n",
        "2.  Posterior normal distribution\n",
        "3.  Posterior inverse Gamma distribution\n"
      ],
      "metadata": {
        "id": "qiBUyZnXM0Wc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Function 1: regime-switching Kalman filter\\\n",
        "input:\n",
        "1.  parameters generated from last step's multimove Gibbd sampling\n",
        "including $\\Phi(L), \\Psi_i(L), \\mu_0,\\mu_1,\\sigma_i \\lambda_i, i= 1,2,3,4$\n",
        "2.  series $\\Delta y_{it}, i=1,2,3,4$\n",
        "\n",
        "output:\n",
        "1. mean & variance of $\\Delta \\zeta_{t|t} , t=1, ⋯⋯, T-1, T$\n",
        "2. steady-state Kalman gain\n",
        "\n",
        "\n",
        "---\n",
        "\\begin{equation}\n",
        "\t\\Delta y^*_t=H^*\\zeta_t+\\epsilon_t\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\t\\zeta_t=\\tilde M^*_{s_t}+F^*\\zeta_{t-1}+u^*_t\n",
        "\\end{equation}\n",
        "\n",
        "---\n",
        "For the fuction, we have 9 input:\n",
        "1. N: length of input data\n",
        "2. true4D: the input time series data with 4 dimension, which is $\\Delta y^*_t$ in above equation\n",
        "3. R: measurement noise covariance martix with variance of $\\epsilon_{it}$ on the diagonal\n",
        "$R=\\begin{bmatrix}\n",
        "\\sigma^2_1&0&0&0\\\\\n",
        "0&\\sigma^2_2&0&0\\\\\n",
        "0&0& \\sigma^2_3&0\\\\\n",
        "0&0&0&\\sigma^2_4\n",
        "\\end{bmatrix}\n",
        "$\n",
        "4. Q_kal: the covariance matrix of $u^*_t$, the size of Q_kal depends on the length of vector $\\zeta_t$. if $\\zeta_t$ is a 1*k vector, then size of Q_kal is $k*k$.\n",
        "5. P: the covariance of $\\zeta_t$, this is just initialization, P would get updated during the process. We can simply set as indentity matrix or or initialization to give Kalman filter a better prior.\n",
        "6. H: measurement matrix. It maps the state variables into the measured outputs, simply speaking, it's about using input series $\\Delta y^*_t$ to measure $\\zeta_t$\n",
        "7. A (or F in the equation above): state transition matrix, it connects the two vector by $\\xi_t=F*\\zeta_{t-1}+Q_{kal}$\n",
        "8. K: Kalman gain, this is just initialization, K would get updated in the process. We can set K as a zeros matrix with the same size as H.T (transposed H)\n",
        "9. Mu_st: vector of state-dependent element, correspondent with $\\tilde M^*_{s_t}$ in the equation\n",
        "\n",
        "About $\\zeta_t$: This is unknown. Mean and variance of $\\xi_t$ is what we are going to estimate through Kalman filter.\n",
        "\n",
        "Also: not all element inside the $\\xi_t$ are going to be used in the whole iteration. $\\xi_t$ might be a very long matrix such as $\\zeta_t=\\begin{bmatrix}\n",
        "\t\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2} \\\\ \\Delta c_{t-3}\\\\ \\Delta c_{t-4}\\\\ \\Delta c_{t-5}\n",
        "\\end{bmatrix}$, but in the end we only need $\\Delta c_t$ which is the first element. This will be given more detailed explainations in the full algorithm part with examples.\n",
        "\n",
        "Before running Kalman filter, we should also prepare empty matrix to save the output which is the mean and variance of $\\zeta_t$ ( or some specific rows of $\\zeta_t$).\n",
        "\n",
        "\n",
        "---\n",
        "Kalman filter has 5 equations: First 2 equations are called the process of predicting, later 3 equations are called the proecess of filtering or correction.\n",
        "\n",
        "Initial values are required for the first iteration of kalman filter. For the first input $t=1$, we assume mean value $\\zeta_{1|1}=0$ and variance $P_{1|1}$ be a diagonal matrix with 0.1 on the diagonals. Choice of initial value depends on the user but it would be better to set some reasonable initial. Very wrong initialization could cause trouble for other blocks of the MCMC algorithm in actual tests.\n",
        "\n",
        "\n",
        "t iterates from t=2 to t=T, we should provide the\n",
        "\n",
        "For each iteration t, we use t-1 as input. $\\zeta_{t|t}$ from last iteration would become $\\zeta_{t-1|t-1}$ for next iteration. $P_{t|t}$ from last iteration would become $P_{t-1|t-1}$ for next iteration.\n",
        "\n",
        "---\n",
        "\n",
        "Kalman equation 1:\n",
        "\\begin{equation}\n",
        " \\zeta_{t|t-1}=F^*\\zeta_{t-1|t-1}+\\tilde M^*_{s_t}\n",
        " \\end{equation}\n",
        " Equation 1 is the process of **predicting** mean value of $\\zeta_t $ from $\\zeta_{t-1}$.  $\\  \\zeta_{t|t-1}=E[\\zeta_t|t-1]=E[F*\\zeta_{t-1}+\\phi(L)\\mu_{st}+v_t|t-1]=F*\\zeta_{t-1}$ as $\\mu_{st}$ is imposed mean zero in the setting according to Kim and Nelson (1998) and $v_t$ is iid noise of time t.\n",
        "\n",
        "   This is extended version of normal state-space Kalman filter as we added state-dependent element $M_{st}$ in the first kalman filter equation. For normal kalman filter, first Kalman equation is $\\zeta_{t|t-1}=F^*\\zeta_{t-1|t-1}$.\n",
        "\n",
        "---  \n",
        "Kalman equation 2:\n",
        " \\begin{equation}\n",
        " P_{t|t-1}=F^*P_{t-1|t-1}F^{*^T}+Q\n",
        " \\end{equation}\n",
        " Equation 2 is the process of **predicting** variance of $\\zeta_t $ from $\\zeta_{t-1}$. $Q$ is the variance matrix of process noise $[v_t,\\ v_{t-1},\\ 0]'$.  \n",
        "\n",
        "---\n",
        "Kalman equation 3:\n",
        "\\begin{equation}\n",
        "\tK(t)= P_{t|t-1}H^{*^T}[H^*P_{t|t-1}H^{*^T}+R]^{-1}      \n",
        "\\end{equation}\n",
        "Equation 3 is the process of **filtering** or upgrading. Through this equation we can obtain the value of Kalman gain $K$. Kalman gain would stablize after a few iterations (which is called as steady-state Kalman gain). This is caused by the stablized variance matrix $P$ after a few iterations.\n",
        "  \n",
        "---\n",
        "Kalman equation 4:\n",
        "\\begin{equation}\n",
        " \\zeta_{t|t}= \\zeta_{t|t-1}+K(t)([\\Delta y^*_t-H^*\\zeta_{t|t-1}]\n",
        "\\end{equation}\n",
        "Equation 4 is the process of **filtering** mean value of $\\zeta_t$ by adding the filtered error term at time t.\n",
        "  \n",
        "---\n",
        "Kalman equation 5:\n",
        "\\begin{equation}\n",
        "\tP_{t|t}=[I-K(t)H^*]P_{t|t-1}\n",
        "\\end{equation}\n",
        "where $I$ is identity matrix.  \n",
        "Equation 5 is the process of **filtering** variance of $\\zeta_t$. $P$ would stablize with the stablization of Kalman gain after a few iterations.\n",
        "  "
      ],
      "metadata": {
        "id": "pLjSB1i3YxUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "Function 2: the process of adding t+1 data to the information set after running kalman filter\n",
        "\n",
        "This process is from step 2 of appedix A1 in Kim $\\&$ Nelson (1998). After running the Kalman filter to generate mean and variance of $\\Delta c_t$ conditional on t, which is $\\Delta c_{t|t}$, Kim applied t+1 data to update the mean & variance of $\\Delta c_{t|t}$.\n",
        "\n",
        "Reason for using t+1 data to improve estimation:\n",
        "1.  From information set side, for data at time t, as Kalman filter is a forward iteration, data from 1 to t-1 are already in the information set for $\\Delta c_{t|t}$. If we want something new, some extra information, it's only available from the rightside of t: t+1...,T-1,T. Considering t+1 data is known from Kalman filter, we can now treat t+1 data as observations.\n",
        "2.  From Kalman filter side, kalman filter would smooth the series and reduce the noise, but at the same time, kalman filter would make the series generated from $\\Delta y^*_t$ less informative. This might be kind of vague, one example is: If we don't do this process after the Kalman filter, the sharp decrease in index caused by great recession (such as 2008, 2020) would get smoothed by Kalman filter. It would make 2008 or 2020 look like relatively mild recessions which is not proper. This 't+1' process can effectively bring back the useful information (such as strength of recession) to the generated series $\\Delta c_t$.\n",
        "3.  From final effect side, the variance of each data point of $\\Delta c_t$ would decrease after this process because the whole system gets more informative after including t+1 data as extra observations into information set. The final sample of $\\Delta c_{t|t+1}$ generated from new mean and variance would be less noisy compared with $\\Delta c_{t|t}$ generated from former mean and variance.\n",
        "\n",
        "---\n",
        "Reference for this updating process:chapter 13.6 http://mayoral.iae-csic.org/timeseries2021/hamilton.pdf  or see step2 of appendix A2 from Kim and Nelson (1998)\n",
        "\n",
        "---\n",
        "Update equations:\n",
        "\\begin{equation}\n",
        "\t\\zeta_{t|t,\\Delta c_{t+1}}=\\zeta_{t|t}+V_{t|t}F^*(1)\\eta_t/R_t\n",
        "\\end{equation}  \n",
        "\n",
        "\\begin{equation}\n",
        "\tV_{t|t,\\Delta c_{t+1}}=V_{t|t}-V_{t|t}F^*(1)F^*(1)'V_{t|t}'/R_t\n",
        "\\end{equation}\n",
        "\n",
        "where $\\eta_t=\\Delta c_{t+1}-\\Phi(L)\\mu_{t+1}-F^*(1)\\zeta_{t|t}$ and $R_t=F^*(1)V_{t|t}F^*(1)'+var[u^*_{t+1}(1)]$. $F^*(1)$ and $u^*_{t+1}(1)$ are first row of $F^*$ and $u^*_{t+1}$.\n",
        "$ F^* =\\begin{bmatrix}\n",
        "\t\t\\phi_1 & \\phi_2 & 0 & 0 & 0 & 0 \\\\\n",
        "1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 1 & 0\n",
        "\t\\end{bmatrix}\n",
        "$, so $F^*(1)=\\begin{bmatrix}\n",
        "\t\t\\phi_1 & \\phi_2 & 0 & 0 & 0 & 0\n",
        "        \\end{bmatrix}$\n",
        "       \n",
        "       \n",
        "$\\zeta_{t|t}=\\begin{bmatrix}\n",
        "\t\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2} \\\\ \\Delta c_{t-3}\\\\ \\Delta c_{t-4}\\\\ \\Delta c_{t-5}\n",
        "\\end{bmatrix}$\n",
        "$\n",
        "V_{t|t}=\\begin{bmatrix}\n",
        "\tVar(\\Delta c_t)\\\\ Var(\\Delta c_{t-1}) \\\\ Var(\\Delta c_{t-2}) \\\\ Var(\\Delta c_{t-3})\\\\ Var(\\Delta c_{t-4})\\\\ Var(\\Delta c_{t-5})\n",
        "\\end{bmatrix}\n",
        "$\n",
        "$var[u^*_{t+1}(1)]=var(v_{t+1})=1$ as $v_t\\sim iid N(0,1)$\n"
      ],
      "metadata": {
        "id": "YLOAf9leYxgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "Function 3:calculating posterior mean & variance for normal distribution\n",
        "\n",
        "---\n",
        "Input:\n",
        "prior mean & variance (fixed during the whole process of MCMC)\n",
        "a & b: these two depends on the specific setting of each parameter\n",
        "\n",
        "Output:\n",
        "posterior mean & variance of the parameter\n",
        "\n",
        "---\n",
        "posterior mean = ((prior var$)^{-1}$+a$)^{-1}* $ ((prior var$)^{-1}*$ (prior mean) + b )\n",
        "\n",
        "posterior var  = ((prior var$)^{-1}$+a$)^{-1}$\n",
        "\n",
        "---\n",
        "In the code, we wrote seperate code for mean & variance in scalar and matrix form. The math part is same but we would use inverse matrix function which doesn't fix scalars."
      ],
      "metadata": {
        "id": "JFWN1Wn7Y6JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        " Function 4: generating sample of variance from posterior inverse distribution\n",
        "\n",
        "---\n",
        "Input: a & b\n",
        "\n",
        "a is Shape Parameter : It influences the shape of the distribution. Higher values of a will make the distribution peak more to the right of the origin, concentrating more probability mass on higher values.\n",
        "\n",
        "b is Scale Parameter: It is a positive scalar that stretches or shrinks the distribution along the x-axis. Larger values of b will spread the distribution out, placing more probability mass over a wider range of values.\n",
        "\n",
        "Here our input a & b are posterior for inverse gamma distribution.\n",
        "\n",
        "Output: variance sample generated from posterior inverse gamma distribution\n",
        "In the programming, if we choose to use gamma to generate random sample, the function should be 1/IG(a1,/1/b1) for $\\sigma^2\\sim IG(a1,b1)$ or we can directly use inverse gamma function from the scipy package, code is invgamma.rvs(a,b)."
      ],
      "metadata": {
        "id": "8zz4w-E5ZCos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm, gamma, beta\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy.random import normal\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Function 1. Kalman filter (regime-switching)\n",
        "# Compared with common Kalman filter, there is one extra state-dependent term: Phi(L)*mu_st\n",
        "def Kalman(R,H,A,Q_kal,true4D,Mu_st,N,P,K):\n",
        "    k = 0\n",
        "    for t in range(N-1):\n",
        "        k += 1\n",
        "        x[:, k] = np.dot(A, x[:, k-1])+Mu_st[:,k]    # Kalman equation 1, Mu_st is the vector with state-dependent element\n",
        "                                                     # x[:,0] is vector of zeros as initial input,\n",
        "                                                     # this would be set before running Kalman filter in the MCMC\n",
        "        P = np.dot(A, np.dot(P, A.T)) +Q_kal  # Kalman equation 2\n",
        "        K = np.dot(P, np.dot(H.T, np.linalg.inv(np.dot(H, np.dot(P, H.T)) + R)))  # Kalman equation 3\n",
        "        x[:, k] = x[:, k] + np.dot(K, (true4D[:, k] - np.dot(H, x[:, k])))  # Kalman equation 4\n",
        "        P = np.dot((np.eye(P.shape[0]) - np.dot(K, H)), P)  # Kalman equation 5\n",
        "        vct[0, k] = P[0, 0]  # save variance of delta_ct\n",
        "    return K, x, vct\n",
        "\n",
        "# Function 2. the process of adding t+1 data to the information set after running kalman filter\n",
        "# From this process, we will obtain mean & variance of delta c_{t|t+1}\n",
        "\n",
        "# The order of updating is from back to front, using data from T to update T-1 data,then update T-2...,3,2,1\n",
        "\n",
        "def afterkal(x,vct,Q_kal,phi,c):# add t+1 data to information set\n",
        "    for ii in range(N-2):  # this loop is just for xi_t and V_t conditional on delta_ct+1 for t=1,2,...,T-1\n",
        "\n",
        "        # generate eta_t and R_t first\n",
        "        eta = c - phi[0]*x[0, N-ii-2] - phi[1]*x[0, N-ii-3] - phimu_st[N-ii-1]  # first input c is delta c_{T|T}\n",
        "                                                                                # start from 2nd iteration\n",
        "                                                                                # c is delta c_{t|t+1}\n",
        "        Rt = phi[0]**2*vct[0, N-ii-2]+phi[1]**2*vct[0,N-ii-3]+var_vt\n",
        "\n",
        "\n",
        "        # we only use first row of the mean and variance vector\n",
        "        # so I calculated the first element of each mean & variance directly\n",
        "\n",
        "        xi[0, N-ii-2] = x[0, N-ii-2] + (phi[0]*vct[0, N-ii-2]+phi[1]*vct[0,N-ii-3])*eta/Rt\n",
        "        V[0, N-ii-2] = vct[0, N-ii-2] - (phi[0]**2 * (vct[0,N-ii-2]**2)+phi[1]**2*(vct[0,N-ii-3]**2)) / Rt\n",
        "\n",
        "        c = normal(xi[0, N-ii-2], np.sqrt(V[0, N-ii-2]))\n",
        "    # We only updated data from 1 to T-1,\n",
        "    # mean & variance of delta c_T didn't get updated, it's still data generated from Kalman filter\n",
        "    V[0, N-1] = vct[0, N-1]# V_T didn't get upgraded, it's still from step 1\n",
        "    xi[0, N-1] = x[0, N-1]# xi_T didn't get upgraded, it's still from step 1\n",
        "    return xi,V\n",
        "\n",
        "# Function 3. calculating posterior mean & variance for normal distribution\n",
        "def posteriornormal(mean,var,a,b):\n",
        "    # input mean & variance are priors, they are fixed during the whole process\n",
        "\n",
        "    # if mean & variance are scalar:\n",
        "    if np.isscalar(mean) and np.isscalar(var):\n",
        "        mean = 1/(1/var+a) * (mean/var + b )\n",
        "        var  = 1/(1/var+a)\n",
        "\n",
        "    #if mean & variance are matrix\n",
        "    else:\n",
        "        mean = np.linalg.inv(np.linalg.inv(var)+a) @ (np.linalg.inv(var) @ mean + b )\n",
        "        var  = np.linalg.inv(np.linalg.inv(var)+a)\n",
        "    if np.isnan(mean).any(): # if NaN shows up,reset to initial value\n",
        "        mean = np.zeros(mean.shape)\n",
        "        var = np.eye(var.shape[0])\n",
        "    return mean, var\n",
        "\n",
        "\n",
        "\n",
        "# Function 4. Generating random sample for posterior inverse gamma distribution\n",
        "def posteriorIG(a,b):\n",
        "    #input a,b here are posterior values, priors of sigma_i are all IG(0,0) here\n",
        "    sig=1 / np.random.gamma(a, 1/b)\n",
        "    return(sig)"
      ],
      "metadata": {
        "id": "pEfZ3W2SZGMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################################\n",
        "#          pre-setting\n",
        "##########################################################################\n",
        "import numpy as np\n",
        "from scipy.stats import norm, gamma, beta\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy.random import normal\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "# Remove warning message of time series from each iteration\n",
        "def custom_warning_handler(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "warnings.showwarning = custom_warning_handler\n",
        "\n",
        "##########################################################################\n",
        "#          iteration setting\n",
        "##########################################################################\n",
        "iter = 10000 # number of iterations\n",
        "burn = 2000  # we burn the first 2000 iterations and sampling from the later 8000 iterations in the MCMC\n",
        "             # If we find the samples still can't converge after 2000 iterations,\n",
        "             # burn and whole iteration number can be modified.\n",
        "             # Another common choice is iteration 20000 times and burn the first 10000 iterations.\n",
        "\n",
        "             # A higher number of burns would make the results more stable & convincing\n",
        "             # but it would also make the program much slower\n",
        "\n",
        "##########################################################################\n",
        "#          input data\n",
        "##########################################################################\n",
        "delta_y_mean = np.zeros((4,1))\n",
        "for i in range(4):\n",
        "    delta_y_mean[i,0] = np.mean(kalpre1.iloc[:, i]) # mean value of Y_it\n",
        "                                                    # This will be used in generating delta in appendix 7\n",
        "\n",
        "kalpre=kalpre1.values\n",
        "np.random.seed(198011) #fix randomness\n",
        "N = kalpre.shape[0] #length of input\n",
        "\n",
        "\n",
        "Y = np.transpose(kalpre[:,0:4]) # delta Y_{1t} to delta Y_{4t}\n",
        "                                # form delta_y_t, delta_y_t-1, and delta_y_t-2\n",
        "                                # delta y_it=delta Y_it-E(delta Y_it), remove the mean value\n",
        "y = np.zeros((4, N))\n",
        "y1lag = np.zeros((4, N))\n",
        "y2lag = np.zeros((4, N))\n",
        "\n",
        "for i in range(4):\n",
        "    y[i, :] = kalpre[:, i ].T-np.mean(kalpre[:, i ]) # equation 6 in the 'Modeling of the algorithm' in notes above\n",
        "    y1lag[i, :] = kalpre[:, i + 4].T-np.mean(kalpre[:, i+4 ])\n",
        "    y2lag[i, :] = kalpre[:, i + 8].T-np.mean(kalpre[:, i+8 ])\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# storage space for kalman filter output\n",
        "##########################################################################\n",
        "x = np.zeros((3, N)) # empty matrix to save mean value of xi_t\n",
        "xi=np.zeros((1,N))   # mean of delta c_{t|t+1}\n",
        "V=np.zeros((1,N))    # variance of delta c_{t|t+1}\n",
        "mu_st=np.zeros(N)    # regime-switching element mu_st\n",
        "phimu_st=np.zeros(N) # regime-switching element with lags for kalman filter: phi(L)*mu_st,\n",
        "                     # phimu_st is input for Kalman filter equation 1\n",
        "new_delta_ct= np.zeros((N, 1)) # delta c_{t|t+1}\n",
        "\n",
        "\n",
        "# Form noise matrix Q_kal, correpodent with vector u^*_t=[var_vt 0 0 0 0 0]'\n",
        "var_vt=1             # iid variance in equation (3), vt~iid N(0,1)\n",
        "                     # scale of new economic index is up to the choice of v_t's variance\n",
        "Q_kal = np.zeros((6,6))\n",
        "Q_kal[0,0]=var_vt\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# Gibbs sampling\n",
        "##########################################################################\n",
        "\n",
        "Nn=kalpre.shape[0]-2            # length of input for Gibbs sampling\n",
        "rrighta3 = np.zeros((1, Nn - 2))#this matrix is for calculating posterior distribution of lamda\n",
        "rrighta4 =np.zeros((Nn-5,4))    # for lamda_40,41,42,43\n",
        "Q = np.zeros((Nn - 4, 2))\n",
        "Qstar = np.zeros((Nn - 4, 2))\n",
        "mut = np.zeros(Nn)              # mu_st used generating posterior distribution of phi, data length N-2\n",
        "\n",
        "meanlamda=np.zeros(4)\n",
        "varlamda=np.zeros(4)\n",
        "lamda=np.zeros(4)\n",
        "sig=np.zeros(4)\n",
        "vvarpsi=np.zeros((2,4))\n",
        "mmeanpsi=np.zeros((2,4))\n",
        "\n",
        "# Input for gibbs sampling\n",
        "inputkal1 = np.full((Nn,7), np.nan)\n",
        "# Fill the columns 4 to 7 with the data y_it, i=1,2,3,4\n",
        "inputkal1[:,3:7] = (y[:,2:N]).T\n",
        "# name of each column: 1. delta c_t, 2. delta c_t-1 3. delta c_t-2 thses will be updated every iteration\n",
        "#                      4 to 7: delta y_it, i=1,2,3,4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# user-choose prior\n",
        "##########################################################################\n",
        "\n",
        "# variance and mean prior\n",
        "varlamda4p = np.eye(4)       # prior for lamda_4,41,42,43\n",
        "meanlamda4p = np.array([[0],[0],[0],[0]])\n",
        "varlamdap=1                  # prior for lamda_1,2,3\n",
        "meanlamdap=0\n",
        "varpsip = np.eye(2)\n",
        "meanpsip = np.zeros((2, 1))\n",
        "sig_a=(Nn-2)/2               # when calculating sigma, length of applied series is Nn-2 for i=1,2,3\n",
        "                             # for i=4, length of input data is N-5, this will be set in the itertaion independently\n",
        "                             # sigma_1~IG(sig1a,sig1b) inverse gamma distribution\n",
        "                             # prior is IG(0,0),\n",
        "                             # because posterior is a fixed number : shape parameter prior+T/2 which is just T/2,\n",
        "                             # so we just set sig_i_a as (Nn-2)/2 here\n",
        "                             # T is the lenth of input series, here the length is Nn-2\n",
        "                             # if want different prior such as m, let sig1a=...=sig5a=m+T/2\n",
        "varmup=varphip = np.eye(2)\n",
        "meanmup=meanphip = np.zeros((2,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# form storage space\n",
        "##########################################################################\n",
        "# We set some empty matrices here to save values from each iteration\n",
        "# After the 2000 burn-in period, we need to save value for later 80000 iterations\n",
        "\n",
        "vlamda=np.zeros((iter-burn,7)) #lamda_1,2,3 lamda_4_{0,1,2,3}\n",
        "mlamda=np.zeros((iter-burn,7))\n",
        "bsig=np.zeros((iter-burn,4))\n",
        "vpsi=np.zeros((iter-burn,8))\n",
        "mpsi=np.zeros((iter-burn,8))\n",
        "mphi=np.zeros((iter-burn,2))\n",
        "vphi=np.zeros((iter-burn,2))\n",
        "mmu=np.zeros((iter-burn,2))\n",
        "vmu=np.zeros((iter-burn,2))\n",
        "mmu1 = np.zeros(iter-burn) # sampling of mu0 is an independent process, better to seperate\n",
        "mmu2 = np.zeros(iter-burn)\n",
        "vmu1 = np.zeros(iter-burn)\n",
        "vmu2 = np.zeros(iter-burn)\n",
        "delta=np.zeros(iter-burn)\n",
        "transition_prob_p=np.zeros(iter-burn)\n",
        "transition_prob_q=np.zeros(iter-burn)\n",
        "delta=np.zeros(iter-burn)\n",
        "regime=np.zeros((N-4, 1))\n",
        "sample_ct=np.zeros((N, 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "# Initialization\n",
        "##########################################################################\n",
        "lamda=0.5*np.ones(3) # lamda_1,2,3\n",
        "lamda4=np.zeros(4)   # lamda_40,41,42,43\n",
        "lamda4[0]=0.5\n",
        "sig=0.2*np.ones(4)   # sigma^2_i, i=1,2,3,4\n",
        "phi=np.zeros(2)\n",
        "psi=np.zeros((4,2))\n",
        "mu0=-2\n",
        "mu1=2.5\n",
        "\n",
        "q = 0.9              # transition probability, recession to recession: q = P(S_t=0|S_t-1=0)\n",
        "p = 0.9              # transition probability, expansion to expansion: p = P(S_t=1|S_t-1=1)\n",
        "\n",
        "##################################  generate initial St based on transition probabilties p,q\n",
        "def generate_markov_chain(p, q, initial_state, N):\n",
        "    # Initialize the Markov Chain\n",
        "    markov_chain = np.zeros(N)\n",
        "    markov_chain[0] = initial_state\n",
        "\n",
        "\n",
        "    for i in range(1, N):\n",
        "        if markov_chain[i-1] == 1:\n",
        "            markov_chain[i] = np.random.choice([0, 1], p=[1-p, p])\n",
        "        else:\n",
        "            markov_chain[i] = np.random.choice([0, 1], p=[q, 1-q])\n",
        "\n",
        "    return markov_chain\n",
        "initial_state = 0  # initial state\n",
        "St = generate_markov_chain(p, q, initial_state, N) # St=0 for recession, St=1 for expansion\n",
        "SSt=St\n",
        "\n",
        "mu_st = mu0 * np.ones(N) + mu1 * np.ones(N) * St   # mu_st= mu_0+ mu_1*St\n",
        "phimu_st = (np.array(mu_st[2:N])                   # Phi(L)*mu_st= (1-phi_1*L-phi_2*L^2)*mu_st\n",
        "            - np.array(mu_st[1:N-1]* phi[0])\n",
        "            - np.array(mu_st[0:N-2]* phi[1]))\n",
        "phimu_st = np.array(phimu_st)\n",
        "phimu_st = np.insert(phimu_st, [0, 0], phimu_st[0])# As Phi(L)*mu_st has two lags\n",
        "                                                   # there will be 2 data loss at the beginning\n",
        "                                                   # we fill those two data points by 0\n",
        "                                                   # so for beginning two points of kalman filter\n",
        "                                                   # We run the Kalman filter as non-regime-switching Kalman filter\n",
        "                                                   # as regime-switching element Phi(L)*mu_st=0\n",
        "Mu_st=np.zeros((6,N))\n",
        "Mu_st[0,:]=phimu_st                                # form M^{~*}_{st}to fit the input size of kalman filter\n",
        "                                                   # other parts are just 0s\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# initial setting finishes here\n",
        "##############################################################################"
      ],
      "metadata": {
        "id": "Yj3wI7_8ZPzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Details of the algorithm**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In Gibbs sampling, we always use the most up-to-date information as input for each block of the Bayesian algorithm.\n",
        "\n",
        "There are three main blocks: the Kalman filter, Hamilton's (1989) basic filter, and parameter estimation through multi-move Gibbs sampling. We will provide details on each block and explain how to connect them iteratively.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "First step: Kalman filter\\\n",
        "input:\n",
        "1.  parameters generated from last step's multimove Gibbd sampling\n",
        "including $\\Phi(L), \\Psi_i(L), \\mu_0,\\mu_1,\\sigma_i \\lambda_i, i= 1,2,3,4,5$\n",
        "2.  series $\\Delta y_{it}, i=1,2,3,4,5$\n",
        "\n",
        "output: mean $\\&$ variance of $\\Delta c_{t|t} , t=1, ⋯⋯, T-1, T$\n",
        "\n",
        "After Kalman filter, we make use of t+1 data of $\\Delta c_{t|t}$ to update $\\Delta c_{t|t}$ based on Kalman filter equations.\n",
        "\n",
        "At the end of step 1: we get $\\Delta c_{t|t+1}$, $t=1,\\cdots, T-1, T$.\n",
        "\n",
        "---\n",
        "\\begin{equation}\n",
        "\t\\Delta y^*_t=H^*\\zeta_t+\\epsilon_t\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\t\\zeta_t=\\tilde M^*_{s_t}+F^*\\zeta_{t-1}+u^*_t\n",
        "\\end{equation}\n",
        "which can be expanded as  \n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "\t\\Delta y^*_{1t}\\\\ \\Delta y^*_{2t}\\\\ \\Delta y^*_{3t}\\\\ \\Delta y^*_{4t}\n",
        "\\end{bmatrix}\n",
        "=\\tiny\\begin{bmatrix}\n",
        "\t\\lambda_1&  -\\lambda_1\\psi_{11}&-\\lambda_1\\psi_{12}&0&0&0\\\\\n",
        "\t\\lambda_2& -\\lambda_2\\psi_{21}& -\\lambda_2\\psi_{22}&0&0&0\\\\\n",
        "\t\\lambda_3& -\\lambda_3\\psi_{31}& -\\lambda_3\\psi_{32}&0&0&0\\\\\n",
        "\t\\lambda_4&  -\\lambda_4\\psi_{41} + \\lambda_{41}&\\ \\ -\\lambda_4\\psi_{42} - \\lambda_{41}\\psi_{41} + \\lambda_{42}&\\ \\ -\\lambda_{41}\\psi_{42} - \\lambda_{42}\\psi_{41} + \\lambda_{43}&\\ \\ -\\lambda_{42}\\psi_{42} - \\lambda_{43}\\psi_{41}&\\ \\ -\\lambda_{43}\\psi_{42}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\t\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2} \\\\ \\Delta c_{t-3}\\\\ \\Delta c_{t-4}\\\\ \\Delta c_{t-5}\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "\t\\epsilon_{1t}\\\\\n",
        "\t\\epsilon_{2t}\\\\\n",
        "\t\\epsilon_{3t}\\\\\n",
        "\t\\epsilon_{4t}\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\t\\begin{bmatrix}\n",
        "\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2}\\\\ \\Delta c_{t-3} \\\\ \\Delta c_{t-4} \\\\ \\Delta c_{t-5}\n",
        "\t\\end{bmatrix}\n",
        "\t=\n",
        "\t\\begin{bmatrix}\n",
        "\t\t\\Phi(L)\\mu_{s_t}\\\\\n",
        "\t\t0\\\\0\\\\0\\\\0\\\\\n",
        "\t\t0\n",
        "\t\\end{bmatrix}\n",
        "\t+\\begin{bmatrix}\n",
        "\t\t\\phi_1 & \\phi_2 & 0 & 0 & 0 & 0 \\\\\n",
        "1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 1 & 0\n",
        "\t\\end{bmatrix}\n",
        "\t\\begin{bmatrix}\n",
        "\t\\Delta c_{t-1}\\\\ \\Delta c_{t-2} \\\\ \\Delta c_{t-3}\\\\ \\Delta c_{t-4} \\\\ \\Delta c_{t-5} \\\\ \\Delta c_{t-6}\n",
        "\t\\end{bmatrix}\n",
        "\t+\n",
        "\t\\begin{bmatrix}\n",
        "\t\tv_t\\\\ 0 \\\\ 0\\\\ 0 \\\\ 0\\\\ 0\n",
        "\t\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Before starting Kalman filter, we form all the matrix above. Also, $\\Delta y^*_{it}=\\Psi_i(L)*\\Delta y_{it}$, we use new $\\Psi_i(L)$ estimated from last iteration to form new $\\Delta y^*_{it}$ at the beginning of Kalman filter in every iteration.\n",
        "\n",
        "---\n",
        "Why we need such long and useless vector $\\begin{bmatrix}\n",
        " \\Delta c_{t-1} \\\\ \\Delta c_{t-2} \\\\ \\Delta c_{t-3} \\\\ \\Delta c_{t-4} \\\\ \\Delta c_{t-5} \\\\ \\Delta c_{t-6}\n",
        "\t\\end{bmatrix}$ for the second equation?\n",
        "\n",
        "\n",
        "The only valid part is just first row of second equation: $\\Delta c_t=\\Phi(L)*\\mu_{st}+\\phi_1*\\Delta c_{t-1}+\\phi_2*\\Delta c_{t-2}+v_t$. Other rows have exact elements on right and left side such as $\\Delta c_{t-2}=\\Delta c_{t-2}$.\n",
        "\n",
        "Reason:\n",
        "\n",
        "The function of Kalman filter is to estimate mean and variance of $\\zeta_t$, $\\zeta_t=\t\\begin{bmatrix}\n",
        "\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2} \\\\ \\Delta c_{t-3} \\\\ \\Delta c_{t-4} \\\\ \\Delta c_{t-5}\n",
        "\t\\end{bmatrix} $.  \n",
        "$\\zeta_t$ is estimated as a whole by the Kalman filter with all the parameters. We can't just extract the valid part to calculate. The consequence (if we only calculate only the first row of the second equation) would be:\n",
        "\n",
        "Step of Kalman filter:\n",
        "\n",
        "1. for iteration t, we get mean $\\&$ variance of $\\Delta c_{t-1|t-1}$ from $\\zeta_{t-1|t-1}$\n",
        "2. predict mean and variance of $\\Delta c_{t|t-1}$ based on Kalman equation 1$\\&$2 with mean and variance of $\\delta c_{t-1|t-1}$\n",
        "3. form mean vector &. variance matrix of $\\zeta_{t|t-1}=\\begin{bmatrix}\n",
        "\\Delta c_{t|t-1}\\\\ \\Delta c_{t-1|t-2} \\\\ \\Delta c_{t-2|t-3} \\\\ \\Delta c_{t-3|t-4} \\\\ \\Delta c_{t-4|t-5} \\\\ \\Delta c_{t-5|t-6}\n",
        "\t\\end{bmatrix}$\n",
        "4. run Kalman equaton 3,4,5\n",
        "5. next iteration, go back to step 1\n",
        "\n",
        "This would take much more trouble compared with forming a big matrix with some uselss elements inside.\n",
        "\n",
        "But still, the main reason for us not to seprate $\\zeta_t$ is that  $\\zeta_t$ is an unknown variables estimated as a whole from beginning to end.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We iteratively run the algorithm from equation 1 to 5 ( details of Kalman filter equations can be checked at the function notes part) from $t=2$ to $t=N$ and in the end we would obtain the filtered mean and variance of $\\Delta c_{t|t}$ for $t=1\\dots N$ from each iteration.\n",
        "\n",
        "After Kalman filter, we make full use of the information to improve the estimation result of Kalman filter.\n",
        "\n",
        " Take $\\Delta\\ c_{t+1}$ as observation and run a backward iteration to improve the effect of estimation.\n",
        "\n",
        "---\n",
        "\n",
        "We take $\\Delta c_{t+1}$ as extra observation and add  it to information set to lower the variance based on the following equation:\n",
        "\\begin{equation}\n",
        "\tp(\\tilde{\\zeta}_T|\\Delta\\tilde{y}^*_T)=p(\\zeta_T|\\Delta\\tilde{y}^*_T)\\mathop{\\Pi}\\limits^{T-1}_{t=1}p(\\zeta _t|\\Delta\\tilde{y}^*_t,\\zeta _{t+1})\n",
        "\\end{equation}\n",
        "where joint distribution $\\tilde{\\zeta}_T=[\\begin{matrix}\n",
        "\t\\zeta_1 &\\zeta_2& \\cdots &\\zeta_T\n",
        "\\end{matrix}]$' and $\\tilde{y}_T=[\\begin{matrix}\n",
        "\ty_1 &y_2& \\cdots &y_T\n",
        "\\end{matrix}]$'.\n",
        "\\\\ From last step, $\\zeta _{t|t}$ and $V_{t|t}$ is obtained and we generated $\\Delta c_T$. In this step, $\\Delta c_t$ for $t=1,2,\\cdots T-1$ is what we are going to generate. We follow \\citet{kim} and chapter 13 of \\citet{ham2}, the updating equations of adding $\\Delta c_{t+1}$ into information set are:  \n",
        "\n",
        "\\begin{equation}\n",
        "\t\\zeta_{t|t,\\Delta c_{t+1}}=\\zeta_{t|t}+V_{t|t}F^*(1)\\eta_t/R_t\n",
        "\\end{equation}  \n",
        "\n",
        "\\begin{equation}\n",
        "\tV_{t|t,\\Delta c_{t+1}}=V_{t|t}-V_{t|t}F^*(1)F^*(1)'V_{t|t}'/R_t\n",
        "\\end{equation}  \n",
        "\n",
        "where $\\eta_t=\\Delta c_{t+1}-\\Phi(L)\\mu_{t+1}-F^*(1)\\zeta_{t|t}$ and $R_t=F^*(1)V_{t|t}F^*(1)'+var[u^*_{t+1}(1)]$. $F^*(1)$ and $u^*_{t+1}(1)$ are first row of $F^*$ and $u^*_{t+1}$.\n",
        "  \n",
        "  With the new mean $\\zeta_{t|t,\\Delta c_{t+1}}$ and variance $V_{t|t,\\Delta c_{t+1}}$ conditional on $\\Delta c_{t+1}$, we can generate $\\Delta c_t$ for $t=1,2,\\cdots T-1$, we have also got $\\Delta c_T$ from last step. In the end, we get $\\Delta c_{t|t+1}$ for $t=1,2,\\cdots T$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        " **Second step: Hamilton (1989) basic filter**\n",
        "\n",
        "Here we applied MarkovAutoregression package in python. There is prepared Hamilton filter inside the package.\n",
        "We could also use Markovregression package which will be 10 times faster but we will miss many small recessions if we choose this package. It's fast but not informative.\n",
        "\n",
        "Input: series $\\Delta c_{t|t+1}$ from Kalman filter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Setting of Hamilton filter:\n",
        "1.  regime number:2\n",
        "2.  order of autoregression:4 (this could be different for different data) If we don't set order of autoregression properly, there could be two consequences: 1. improper etimation regime probabilities 2. the worst case is that hamilton filter would collapse (happened in actual tests). Hamilton filter can't generate regime probabilties (or just P($recession_t$) is a stochastic process around 0.5, meaningless estiamtion. For UK data, order=2 is the appropriate choice.\n",
        "\n",
        "\n",
        "Output:\n",
        "1. regime probabilities of recession $\\&$ expansion at each time t\n",
        "2. steady state transition probabilities between regimes\n",
        "3. regime series (if at time t, $P(recession)>P(expansion), S_t=0,$ otherwise $S_t=1$\n",
        "\n",
        "After Hamilton basic filter, there is one extra smooth process based on Bayes rule in Kim & Nelson (1998), this is also covered in the MarkovAutoregression package. We don't need to write extra code on this.\n",
        "\n",
        "Hamilton(1989) filter is critial part for this 'regime-switching' algorithm as it exposes the recession/expansion information from input data.\n",
        "\n",
        "One thing needs extra attention is that Hamilton(1989) basic filter can't handle extreme values. The program would collapse if extreme value shows up. There are a few things we can do if this happens.\n",
        "1.  Modify regime numbers. We can try giving extreme situation another indepednt regime. For example, current nodel here uses 2-regime setting, we could try 3 regimes.\n",
        "2.  Apply smoothers to the input data, actually kalman filter/ Kalman smoother has smoothed the data but in our algorithm, we applied the process of using t+1 data to improve estimation after running kalman filter. The good side is this will make input series more informative but the bad side is the new series generated after using t+1 data will break the smooth effect at the same time. So when you face the situation of extreme values, there are two options: smoothed but less informative data or informative data but there are extreme values.\n",
        "3.  Remove the process of using t+1 data after kalman filter. This process would make the input $\\Delta c_t$ more informative but this process can also lead to the existence of extreme values at the same time. Extreme value could cause the collapse of Hamilton (1989) basic filter. Point 2 $\\&$ 3 are talking about similar situation, the process of applying t+1 data is the source of this algorithm, we remove this process or we give extra smooth ( if the Hamilton filter collapses).\n",
        "4.  Romove the most unstable data period (such as 2020 recession in UK data test). Without this period, it's much easier for Hamilton basic filter to work. By the way, if we romove a period of extreme value, the generated recession probability series can be very different according to the actual tests. Very different probability series doesn't mean it's wrong. It's just because information we provided for this ML method gets very different, then the algorithm would tell us a new result based on this new series. The difference is: In former series, we can only see signals of very strong recession, probabilities of normal recession would only be about 0.1, 0.2, but we are expecting it to be over 0.5 as they are also real recessions. If we remove the most strong extreme values, the etimation for these common or weak recession would look better. Consequently, the effect of etimation normal time would be better and more reasonable. But we do care more about those extreme recessions rather than mild ones. For forecast or estimation of normal period, including extreme value would reduce the effictiveness but this may not be an appropriate choice for study of originally unstable stuff such as volatilities and study of exploring recession period/signals like what we are doing.\n",
        "5. We are also looking for a better algorithm that can fulfill the regime-switching related function and can handle extreme value at the same time (something better than Hamilton filter) but haven't find a good one.  \n",
        "\n",
        "Some extra setting after the Hamilton filter:\n",
        "\n",
        "As we set autoregression order=4, there are only N-4 data as output. The 4 missing data point at the beggining of series, we set them all as 1 according to kim's code. So the output St series are: [1,1,1,1, St]\\\n",
        "Would this affect estimation results? Very small influence. 1. For parameter estimation after hamilton filter, we use N-2 data, so only first 2 data are affected by our setting. 2. For Kalman filter, prediction and filtering part of Kalman filter is not accurate at the beginning part, the variance would stablize at around t=10. What's more, after running the code for the first time, we can modify the frist 4 St according to the estimation result of the model (such as [1 0 0 0 St] or [0 1 1 0 St], this would minimize the negative influence.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UHYbruRpZERo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Third step: parameter estimation**\n",
        "\n",
        "input: $\\Delta y_{it}$, $\\Delta c_t$, $S_t$, prior distribution of each parameter\n",
        "\n",
        "output: posterior distribution of each parameter.\n",
        "\n",
        "---\n",
        "Prior distributions for each parameter stay fixed during the process of MCMC.\n",
        "\n",
        "The equation of posterior distribution is calculated through MLE.\n",
        "\n",
        "In each calclulation of posterior distribution, we always use the most up-to-date copy of input.\n",
        "\n",
        "---\n",
        "**Gibbs sampling is seperated into 3 sections:**\n",
        "\n",
        "**Section 1: Generating lamda**\n",
        "\n",
        "\n",
        " Generating $\\lambda _i$, $i=1,2,3,4$\n",
        "Given $\\tilde{S}_T=\\begin{bmatrix}\n",
        "\tS_1& S_2 &\\cdots&S_T\n",
        "\\end{bmatrix}'$ from last step and $\\tilde{c}_T=\\begin{bmatrix}\n",
        "\tc_1& c_2 &\\cdots&c_T\n",
        "\\end{bmatrix}'$.\n",
        "\n",
        "\\begin{equation}\n",
        "\ty^*_{it}=\\lambda_i\\Delta c_t^*+\\epsilon _{it}\n",
        "\\end{equation}\n",
        "where $\\Delta c^*_t=\\Psi_i(L)\\Delta c_t$ and $\\Delta y^*_t=\\Psi_i(L)\\Delta y_t$. $\\Psi_i(L)$ we use in one iteration is the most up-to-date sample generated from last iteration. Define $\\Delta \\tilde{C}^*$ and $\\Delta \\tilde{y}^*_i$ as variable vectors of right-hand-side and left-hand-side individually. We can then generate posterior distribution $\\lambda_i$ by:\n",
        "\n",
        "\n",
        "$$\n",
        "\\lambda_i \\sim N\\left(\\left(\\sigma_i^{-2} \\Delta\\tilde{C}^{\\ast \\prime} \\Delta\\tilde{C}^{\\ast} + X_i^{-1}\\right)^{-1}\\left(\\sigma_i^{-2} \\Delta\\tilde{C}^{\\ast \\prime} \\Delta\\tilde{y}_i^{\\ast} + X_i^{-1} x_i\\right), \\left(\\sigma_i^{-2} \\Delta\\tilde{C}^{\\ast \\prime} \\Delta\\tilde{C}^{\\ast} + X_i^{-1}\\right)^{-1}\\right)\n",
        "$$\n",
        "\n",
        "given prior distribution as\n",
        "\\begin{equation}\n",
        "\t\\lambda_i\\sim N(x_i,X_i)\n",
        "\\end{equation}\n",
        "\n",
        "$\\sigma_i$ used in current iteration is sample generated from last itereation. prior mean $x_i$ is fixed as 0 and prior variance $X_i$ is fixed as 1 for i=1,2,3. prior of $\\lambda_4$ is fixed as $x_i=\\begin{bmatrix}\n",
        "0 \\\\ 0 \\\\ 0 \\\\0 \\end{bmatrix}$, $X_i=\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 1 & 0 \\\\\n",
        "0 & 0 & 0 & 1 \\\\\n",
        "\\end{bmatrix}$\n",
        "When drawing random samples for $\\lambda_4$, we should use multivariate normal. If we use univariate normal (only use diagonal term in covariance to run the np.random.normal() code), this could cause quite different results if the covariance is strong.\n",
        "\n",
        "---\n",
        "\n",
        "**Section 2: Generating $\\Psi_i$ $\\&$ $\\sigma_i^2$, $i=1,2,3,4$**\n",
        "\n",
        "\n",
        "Let $Z_t=y_{it}-\\lambda _i \\Delta c_t=e_{it}$ where $\\lambda _i$ is newly generated from last step.\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\Psi(L)Z_t=\\epsilon_{it}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\tZ_t=\\psi_{i1}Z_{t-1}+\\psi_{i2}Z_{t-2}+\\epsilon_{it}\n",
        "\\end{equation}\n",
        "\n",
        "Define $\\Delta \\tilde{Z}$ and $\\tilde{X}$ as variable vectors of right-hand-side and left-hand-side individually. We can then generate posterior distribution $\\tilde\\Psi_i$ ($\\tilde\\Psi_i=\\begin{bmatrix}\n",
        "\t\\psi_{i1}&\\psi_{i2}\n",
        "\\end{bmatrix}'$) by:\n",
        "$$\n",
        "\\tilde{\\Psi}_i \\sim N\\left(\\left(\\sigma_i^{-2} \\tilde{X}^{\\prime} \\tilde{X}^{\\ast} + \\Pi_i^{-1}\\right)^{-1}\\left(\\sigma_i^{-2} \\tilde{X}^{\\prime} \\tilde{Z}_i^{\\ast} + \\Pi_i^{-1} \\pi_i\\right),\\left(\\sigma_i^{-2} \\tilde{X}^{\\prime} \\tilde{X}^{\\ast} + \\Pi_i^{-1}\\right)^{-1}\\right)\n",
        "$$\n",
        "\n",
        "given prior distribution as\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\lambda_i\\sim N(\\pi _i, \\Pi _i)\n",
        "\\end{equation}\n",
        "\n",
        "$\\pi_i$ is fixed as $\\begin{bmatrix}\n",
        "0 \\\\ 0 \\end{bmatrix}$ and $\\Pi_i $ is fixed as $\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}$ for $i=1,2,3,4$\n",
        "\n",
        "\n",
        "With new $\\tilde\\Psi_i$ from above,\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\sigma^2_i\\sim IG(\\frac{a+T}{2},\\frac{b}{2}+\\frac{1}{2}(\\tilde Z-\\tilde X\\tilde \\Psi_i)'(\\tilde Z-\\tilde X\\tilde \\Psi_i))\n",
        "\\end{equation}\n",
        "\n",
        "given prior $$\\sigma^2 \\sim IG(\\frac{a}{2},\\frac{b}{2})$$\n",
        "where $IG$ means inverse gamma distribution. $a$ and $b$ are known value and we follow Kim $\\&$ Nelson (1998) to set $a=b=0$.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Section 3: Generating $\\Phi$ $\\&$ $\\mu$**\n",
        "\n",
        " Generating $\\mu_0$, $\\mu_1$ and $\\tilde \\phi$\n",
        " To generate\n",
        "$\\tilde\\phi=\\begin{bmatrix}\\phi_1&\\phi_2\\end{bmatrix}'$, let $G_t=\\Delta c_t-\\mu_{s_t}$.\n",
        "\n",
        "\\begin{equation}\n",
        "\tG_t=\\phi_1G_{t-1}+\\phi_2G_{t-2}+v_t\n",
        "\\end{equation}\n",
        "let $\\tilde Q$ and $\\tilde G$ be right-hand-side and left-hand-side variable vector individually.\n",
        "We can generate posterior distribution of $\\Phi$.\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\tilde \\Phi\\sim N((\\tilde Q'\\tilde Q+X^{-1})^{-1}(\\tilde Q'\\tilde G+X^{-1}x),(\\tilde Q'\\tilde Q+X^{-1})^{-1})\n",
        "\\end{equation}\n",
        "given prior as\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\Phi\\sim N(x,X)\n",
        "\\end{equation}\n",
        "With new $\\Phi$ above, we let $G^*=\\Delta c_t-\\phi_1\\Delta c_{t-1}-\\phi_2\\Delta c_{t-2}$, then we can get\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\tG^*=\\mu_0^*+\\mu_1(S_t-\\phi_1S_{t-1}-\\phi_2S_{t-2})+v_t\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mu_0^*=\\mu_0(1-\\phi_1-\\phi_2)$. Let $\\tilde Q^*$ and $\\tilde G^*$ be right-hand-side and left-hand-side variable vector individually. Posterior distribution of $\\tilde \\mu=\\begin{bmatrix}\n",
        "\t\\mu_0^*&\\mu_t\n",
        "\\end{bmatrix}$ from.\n",
        "\n",
        "$\\tilde{\\mu} \\sim N\\left(\\left(\\tilde{Q}^{\\ast\\prime}\\tilde{Q}^{\\ast} + X^{*^{-1}}\\right)^{-1}\\left(\\tilde{Q}^{\\ast\\prime}\\tilde{G}^{\\ast} + X^{*^{-1}}x^{\\ast}\\right), \\left(\\tilde{Q}^{\\ast\\prime}\\tilde{Q}^{\\ast} + X^{*^{-1}}\\right)^{-1}\\right)_{I(\\mu_1>0)}\n",
        "$\n",
        "\n",
        "given prior as\n",
        "\\begin{equation}\n",
        "\t\\mu\\sim N(x^*,X^*)\n",
        "\\end{equation}\n",
        "\n",
        "$x^*_i$ is fixed as $\\begin{bmatrix}\n",
        "0 \\\\ 0 \\end{bmatrix}$ and $X^*$ is fixed as $\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Attention:\n",
        "1. $\\mu_1$ has to be greater than 0, as state 1 represents expansion regime, there should be an positive deviation from the mean growth rate of economic index. if $ \\mu_1<0 $, redo the sampling again until we generate a posive $\\mu_1$ from posterior normal distribution of $\\mu$.\n",
        "\n",
        "2.  The genrated mean & variance of $\\mu_0^*$ is the mean and variance of $\\mu_0*(1-\\phi_1-\\phi_2)$. In order to generate $\\mu_0$ we generate a sample from the distribution of $\\mu_0^*$, then we divided the sample $\\mu_0=\\frac{\\mu_0^*} {1-\\phi_1-\\phi_2}$.\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "Generate $\\delta$\n",
        "Based on  equation 10 and 11 in K$\\&$N(1998), we run the Kalman filter again to generate steady-state Kalman gain $K^*$, then we use the steady-state Kalman gain to generate $\\delta$: \\begin{equation*}\n",
        "\\delta=E_1'[I_k-(I_k-K^*H)F]^{-1}K^*\\Delta \\overline{Y}\n",
        "\\end{equation*}\n",
        "(There was a typo in the equation in Kim's paper, The first $K_k$ in paper (in equation A.27) is actually $I_k$ according to Kim's GAUSS code and original calculation in Stock and Watson (1991).\n",
        "k is the dimension of F,  $\\ \\Delta \\overline{Y}=\\begin{bmatrix} \\Delta \\overline{Y_1}&\\Delta \\overline{Y_2}&\\Delta \\overline{Y_3}&\\Delta \\overline{Y_4}&\\Delta \\overline{Y_5}\\end{bmatrix}' $.\n",
        "Difference with former Kalman filter above: here we used $\\Delta y_i=λ(L)*\\Delta c_t+e_{it}$\n",
        "rather than $\\Psi(L)\\Delta y_i=λ(L)\\Psi(L)\\Delta c_t+\\epsilon_{it}$ and $e_{it}$ would also need to be estimated because $e_{it}$ is not a iid noise, we can't treat it like one and put it into the covariance matrix. So in actual programming, the covariance matrix R (which is correpodent with $\\epsilon_t$) will be a $4*4$ zero-matrix (it was a $4*4$ diagonal matrix with $\\sigma_i^2$ at the diagonals in former Kalman filter).\n",
        "\\begin{equation}\n",
        "\t\\Delta y^*_t=H^*\\zeta_t\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\t\\zeta_t=\\tilde M^*_{s_t}+F^*\\zeta_{t-1}+u^*_t\n",
        "\\end{equation}\n",
        "\n",
        "Where\n",
        "\n",
        "$\\zeta_t=\\begin{bmatrix}\n",
        "\t\\Delta c_t\\\\ \\Delta c_{t-1} \\\\ \\Delta c_{t-2} \\\\ \\Delta c_{t-3}\\\\ e_{1,t-1} \\\\ e_{1,t-2}\\\\ e_{2,t-1} \\\\ e_{2,t-2}\\\\ e_{3,t-1} \\\\ e_{3,t-2}\\\\ e_{4,t-1} \\\\ e_{4,t-2}\n",
        "\\end{bmatrix}$\n",
        "$u^*_t=\\begin{bmatrix}\n",
        "    v_t \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\epsilon_{1t}\\\\ 0\\\\ \\epsilon_{2t}\\\\ 0\\\\ \\epsilon_{3t}\\\\ 0\\\\ \\epsilon_{4t}\\\\ 0\\\\\n",
        "    \\end{bmatrix}\n",
        "$\n",
        "\n",
        "$\\tilde{M^*_{s_t}}=\n",
        "\\begin{bmatrix}\n",
        "    \\Phi(L)*\\mu_{st} \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n",
        "$F^* = \\begin{bmatrix}\n",
        "\\phi_1 & \\phi_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & \\psi_{11} & \\psi_{12} & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & \\psi_{21} & \\psi_{22} & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\psi_{31} & \\psi_{32} & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\psi_{41} & \\psi_{42} \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "$H^* = \\begin{bmatrix}\n",
        "\\lambda_1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "\\lambda_2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "\\lambda_3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
        "\\lambda_4 & \\lambda_{41} & \\lambda_{42} & \\lambda_{43} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "By running Kalman filter for this setting, we want to calculate only one value: steady-state Kalman gain. Kalman gain is generated from Kalman filter equation 3 ($K (t)= P_{t|t-1}H^{*^T}[H^*P_{t|t-1}H^{*^T}+R]^{-1}   $) and value of K will stablize after a few iterations. So if we want to gain steady-state Kalman Gain, we can directly take the Kalman gain from the final iteration.\n",
        "\n",
        "---\n",
        "Generate new economic index $C_t$\n",
        "\n",
        "After obtaining $\\delta$ and $\\Delta c_t$, We can generate the final economic index by following the equation $C_t=\\Delta c_t+C_{t-1}+\\delta$, $t=1,2,\\cdots , T$. We need value $C_0$ to start iteratively generating $C_t$, but the chocie of $C_0$ can be arbitrary. We choose $C_0=40$ here as this will make it easier to have some comparison with original economic index. $C_0$ choice for UK data is 99.\n",
        "\n",
        "Another thing needs extra comment is: the trend of new economic index\n",
        "\n",
        "The trend purely comes from the element $\\delta$, we add one more $\\delta$ every single time in $C_t=\\Delta c_t+C_{t-1}+\\delta$. But scale of $\\delta$ depends on the mean value of $Y_{it}$, $\\delta$ could be very small and there will be almost no trend in the new economic index ( this happens for the UK data result)."
      ],
      "metadata": {
        "id": "scuVGOvRZEW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "#Start iteration here\n",
        "##############################################################################\n",
        "\n",
        "for i in range(iter):\n",
        "###########################################################################\n",
        "########## First part of MCMC------- Kalman Filter\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "# Use the parameters from last iteration\n",
        "# we need new A,R,H\n",
        "\n",
        "\n",
        "# Form state transition matrix A (matrix F in the notes)\n",
        "    top_row = np.array([phi[0], phi[1], 0, 0, 0, 0])\n",
        "    # define the bottom part\n",
        "    bottom_part = np.hstack((np.eye(5), np.zeros((5, 1))))\n",
        "    # stack them vertically\n",
        "    A = np.vstack((top_row, bottom_part))\n",
        "\n",
        "    # diagonal matrix with sigma_i^2 on the diagonal\n",
        "    R = np.diag(sig)\n",
        "\n",
        "    #measurement matrix H\n",
        "    H = np.zeros((4,6))\n",
        "    for m in range(3):\n",
        "      H[m,0]=lamda[m]\n",
        "      H[m,1]=-lamda[m]*psi[m,0]\n",
        "      H[m,2]=-lamda[m]*psi[m,1]\n",
        "    H[3,:] = [                                       lamda4[0], #0 lag\n",
        "                              - lamda4[0]*psi[3,0] + lamda4[1], #1 lag\n",
        "          -lamda4[0]*psi[3,1] - lamda4[1]*psi[3,0] + lamda4[2], #2 lag\n",
        "          -lamda4[1]*psi[3,1] - lamda4[2]*psi[3,0] + lamda4[3], #3 lag\n",
        "          -lamda4[2]*psi[3,1] - lamda4[3]*psi[3,0],             #4 lag\n",
        "          -lamda4[3]*psi[3,1]]                                  #5 lag\n",
        "    H = np.squeeze(H)\n",
        "\n",
        "    # all lamda, phi, and psi come from random sampling from last iteration\n",
        "\n",
        "# form Phi(L)*mu_st\n",
        "    phimu_st=np.zeros(N)\n",
        "    phimu_st = (np.array(mu_st[2:N])\n",
        "            - np.array(mu_st[1:N-1]* phi[0])\n",
        "            - np.array(mu_st[0:N-2]* phi[1]))\n",
        "    phimu_st = np.array(phimu_st)\n",
        "    phimu_st = np.insert(phimu_st, [0, 0], phimu_st[0])\n",
        "    Mu_st=np.zeros((6,N))\n",
        "    Mu_st[0,:]=phimu_st\n",
        "    P = 0.1*np.eye(6)      # initialization of variance matrix of xi_t, it will be updated in the Kalman filter\n",
        "    x = np.zeros((6, N))   # empty matrix to save mean value of xi_t\n",
        "    vct = np.zeros((6, N)) # empty matrix to save variance of xi_t\n",
        "# Construct delta_y* using new psi\n",
        "    ystar = y.T - psi[:, 0].T* y1lag[0:4, :].T- psi[:, 1].T*y2lag[0:4, :].T # delta y* = Psi(L)*y_it\n",
        "    true4D = ystar.T\n",
        "    K=np.zeros(H.T.shape)  # initialization of Kalman gain\n",
        "#    z=np.zeros((4,N))\n",
        "\n",
        "# run Kalman filter\n",
        "    K,x,vct=Kalman(R,H,A,Q_kal,true4D,Mu_st,N,P,K)\n",
        "\n",
        "# generate delta_c_T, the final data point T\n",
        "    c = normal(x[0, N-1], np.sqrt(vct[0, N-1]))\n",
        "\n",
        "# add t+1 data to information set\n",
        "    xi, V= afterkal(x,vct,Q_kal,phi,c) # output: mean and variance of delta c_{t|t+1}\n",
        "\n",
        "########################\n",
        "# generate new delta_ct based on new mean and variance\n",
        "    for iii in range(0, N):\n",
        "        new_delta_ct[iii] = np.mean(normal(xi[0,iii], np.sqrt(V[0,iii]),1000)) # new delta_c_t\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "############# Second step of MCMC-------- Hamilton (1989) basic filter\n",
        "###########################################################################\n",
        "\n",
        "# Use output from Kalman filter to generate regime probability then generate St\n",
        "    data= new_delta_ct.flatten() # the only input for Hamilton filter is delta c_{t|t+1}\n",
        "# Create date range\n",
        "    date_range = pd.date_range(start='1959-09', end='1995-01', freq='M')\n",
        "# Create DataFrame with date range and data\n",
        "    df = pd.DataFrame({'date': date_range, 'delta_ct': data})\n",
        "    df.set_index('date', inplace=True)\n",
        "\n",
        "    mod = sm.tsa.MarkovAutoregression(\n",
        "        df['delta_ct'], k_regimes=2, trend=\"n\", order=4, switching_variance=True\n",
        ")\n",
        "    res = mod.fit()\n",
        "\n",
        "    # generate St, if recession probability<0.5, we would let St=1 (which is expansion regime)\n",
        "    #                                               otherwise St=0\n",
        "    St = (res.smoothed_marginal_probabilities[1] < 0.5).astype(int)\n",
        "    SSt = St.reset_index(drop=True)\n",
        "    # 4 data loss at the beginning of series as we set autoregression order=4\n",
        "    # we set 4 missing points as 1,1,1,1 (expansion), details can be checked from the notes\n",
        "    SSt = np.concatenate(([1,1,1,1], SSt)) # length N\n",
        "    St=SSt[2:N].reshape(-1,1)              # length Nn (which is N-2)\n",
        "    # SSt will be set back to St after Gibbs sampling\n",
        "\n",
        "###########################################################################\n",
        "############# Third step of MCMC-------- Gibbs sampling\n",
        "###########################################################################\n",
        "\n",
        "# Input: St & inputkal1 (delta c_t and delta y_it)\n",
        "# St is ready, we now prepare inputkal1\n",
        "\n",
        "# Fill the first three columns with t, t-1, t-2 data from 'new_delta_ct' from each iteration\n",
        "    for iiii in range(3):\n",
        "        inputkal1[:, iiii] = new_delta_ct[2 - iiii:N - iiii].flatten()   # N-2 values\n",
        "# data length for code below is Nn, (Nn=N-2)\n",
        "\n",
        "#There are 3 sections of Gibbs sampling, correpsonding notes can be found above\n",
        "\n",
        "###########################################################################\n",
        "############# Section 1-------- generate new lamda\n",
        "###########################################################################\n",
        "# posterior distribution of lamda\n",
        "\n",
        "# generate lamda_1,2,3 first\n",
        "    righta3 = inputkal1[:, 0].T\n",
        "    for m in range(3):\n",
        "\n",
        "      # Form delta y_i^*~ = Psi_i(L)*delta y_it\n",
        "      ys=(inputkal1[2:Nn, 3+m] - inputkal1[1:Nn-1, 3+m] * psi[m, 0] - inputkal1[0:Nn-2, 3+m] * psi[m,1]).reshape(-1,1)\n",
        "\n",
        "      # Form delta C^*~   = Psi_i(L)*delta c_t\n",
        "      rrighta=(righta3[2:Nn]- righta3[1:Nn-1] * psi[m, 0] - righta3[0:Nn-2] * psi[m,1]).reshape(-1, 1)\n",
        "\n",
        "      # Generate posterior mean & variance for lamda\n",
        "      meanlamda[m], varlamda[m]= posteriornormal(meanlamdap,varlamdap, rrighta.T@ rrighta/sig[m], rrighta.T@ys/sig[m])\n",
        "\n",
        "      # draw random sample of lamda_i using posterior mean & variance\n",
        "      lamda[m]=np.random.normal(meanlamda[m], np.sqrt(varlamda[m]))\n",
        "\n",
        "      if m==0:#print during the process to check convergence\n",
        "          if i % 100 == 0:  # if the remainder of i divided by 100 is 0, it means i is a multiple of 100\n",
        "              print(\"(delta C*~)'*(delta C*~) for lamda \",m+1,\":\",rrighta.T @ rrighta,\"meanlamda1:\", meanlamda[0])\n",
        "\n",
        "\n",
        "#generate lamda4, lamda_41, lamda_42, lamda_43\n",
        "\n",
        "    # Form delta y_i^*~ = Psi(L)*delta y_it\n",
        "    ys4 = (inputkal1[5:Nn, 6] - inputkal1[4:Nn-1, 6] * psi[3, 0] - inputkal1[3:Nn-2, 6] * psi[3,1]).reshape(-1, 1)\n",
        "    # Form delta C^*~   = Psi(L)*delta c_t\n",
        "    # lamda_4 has 3 lags, Psi(L)*delta y_it^*= (lamda_4+lamda_41*L+lamda_42*L^2+lamda_43*L^3)*Psi_4(L)*delta c_t\n",
        "    # correspodent  delta C^*~ =[Psi_4(L)*delta c_{t};\n",
        "    #                            Psi_4(L)*delta c_{t-1};\n",
        "    #                            Psi_4(L)*delta c_{t-2};\n",
        "    #                            Psi_4(L)*delta c_{t-3}]\n",
        "    for j in range(4):\n",
        "        rrighta4[:,j] = (righta3[5-j:Nn-j]- righta3[4-j:Nn-j-1] * psi[3, 0] - righta3[3-j:Nn-j-2] * psi[3,1])\n",
        "\n",
        "    # Generate posterior mean & variance for lamda_4,41,42,43\n",
        "    meanlamda4, varlamda4= posteriornormal(meanlamda4p,varlamda4p, rrighta4.T@ rrighta4/sig[3], rrighta4.T@ys4/sig[3])\n",
        "\n",
        "    # draw random sample of lamda_4_41,42,43 using posterior mean & variance by multivariate normal\n",
        "    lamda4 = np.random.multivariate_normal(meanlamda4.flatten(), varlamda4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "############# Section 2 -------- generate new Psi_i & sigma_i^2\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "# same as above, generate Psi_i and sigma_i^2 for i=1,2,3 first,\n",
        "# then we will calculate Psi_i and sigma_i for i=4 later\n",
        "\n",
        "    for m in range(3):\n",
        "\n",
        "      # generate Z~ and X~ (for generating posterior Psi)\n",
        "      # Zpsi & Xpsi are just Z~ and X~ in notes of Gibbs sampling\n",
        "      Zpsi=inputkal1[:, 3+m] - inputkal1[:, 0] * lamda[m]   # delta y_it-lamda_i*delta c_t\n",
        "      Xpsi=np.column_stack((Zpsi[1:Nn - 1], Zpsi[:Nn - 2])) # X~ is a 2*(Nn-2) matrix:  [Z_t-1  | Z_t-2]\n",
        "\n",
        "      # generate posterior mean & variance of Psi_i\n",
        "      # Psi_i(L)=psi_{i,1}*L+ psi_{i,2}*L^2\n",
        "      # size of posterior mean: 2*1           size of posterior var: 2*2\n",
        "      meanpsi, varpsi = posteriornormal(meanpsip,varpsip,Xpsi.T @ Xpsi / sig[m],(Xpsi.T @ Zpsi[2:Nn]).reshape(-1, 1) / sig[m])\n",
        "\n",
        "      # draw random sample of Psi_i from posterior normal distribution\n",
        "      psi[m,:]=np.random.multivariate_normal(meanpsi.flatten(), varpsi)\n",
        "\n",
        "      # save the value of posterior mean and variance for each Psi_i\n",
        "      mmeanpsi[:,m]=meanpsi.flatten()   # mean and var sample of psi\n",
        "      vvarpsi[:,m]=np.diag(varpsi).T\n",
        "\n",
        "      # generate sigma_i^2,       still in the for loop\n",
        "      # Xx in code below is X~ * Psi_i\n",
        "      Xx=Xpsi*psi[m,:]\n",
        "\n",
        "      # calculate the posterior scale of sigma_4^2\n",
        "      sig_b=0.5*np.dot(((Zpsi[2:Nn] - Xx[:, 0] - Xx[:, 1]).reshape(-1, 1)).T, (Zpsi[2:Nn] - Xx[:, 0] - Xx[:, 1]).reshape(-1, 1))\n",
        "\n",
        "      # draw random sample of sigma_4^2 from posterior inverse gamma distribution\n",
        "      sig[m]=posteriorIG(sig_a, sig_b) # random sample of sigma_i^2\n",
        "\n",
        "# generate Psi_4 and sigma_4^2\n",
        "\n",
        "    # same process, form Z~ and X~\n",
        "    # Z~   = delta y_4t- lamda_4(L)*delta c_t , lamda_4(L) has 3 lags\n",
        "    Zpsi4 = (inputkal1[3:Nn, 6]\n",
        "          -inputkal1[3:Nn, 0] * lamda4[0]\n",
        "          -inputkal1[2:Nn-1, 0] * lamda4[1]\n",
        "          -inputkal1[1:Nn-2, 0] * lamda4[2]\n",
        "          -inputkal1[0:Nn-3, 0] * lamda4[3]\n",
        "         )\n",
        "\n",
        "    # X is a 2*(Nn-2) matrix:  [Z_t-1  | Z_t-2]\n",
        "    Xpsi4 = np.column_stack((Zpsi4[1:Nn - 4], Zpsi4[:Nn - 5])) # Zpsi4_t-1, Zpsi4_t-2\n",
        "\n",
        "    # generate posterior mean & variance of Psi_4\n",
        "    meanpsi, varpsi = posteriornormal(meanpsip,varpsip,Xpsi4.T @ Xpsi4 / sig[3],(Xpsi4.T @ Zpsi4[2:Nn-3]).reshape(-1, 1) / sig[3])\n",
        "\n",
        "    # draw random sample of Psi_4 from posterior normal distribution\n",
        "    psi[3,:]=np.random.multivariate_normal(meanpsi.flatten(), varpsi)\n",
        "\n",
        "    # save the value of posterior mean and variance for Psi_4\n",
        "    mmeanpsi[:,3]=meanpsi.flatten()\n",
        "    vvarpsi[:,3]=np.diag(varpsi).T\n",
        "\n",
        "    # generate sigma_4^2\n",
        "    # Xx4 in code below is X~ * Psi_4\n",
        "    Xx4 = Xpsi4 * psi[3,:]\n",
        "\n",
        "    # calculate the posterior scale of sigma_4^2\n",
        "    sig_4b =  0.5*np.dot(((Zpsi4[2:Nn-3] - Xx4[:, 0] - Xx4[:, 1]).reshape(-1, 1)).T, (Zpsi4[2:Nn-3] - Xx4[:, 0] - Xx4[:, 1]).reshape(-1, 1))\n",
        "\n",
        "    # draw random sample of sigma_4^2 from posterior inverse gamma distribution\n",
        "    sig[3]=posteriorIG((Nn-5)/2, sig_4b)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "############# Section 3 -------- generate new phi and mu\n",
        "###########################################################################\n",
        "\n",
        "# generate phi\n",
        "\n",
        "    # form G and Q\n",
        "    G = inputkal1[4:Nn, 0] - mut[4:Nn]              # G = delta c_t-mu_st\n",
        "    Q[:, 0] = inputkal1[3:Nn-1, 0] - mut[3:Nn-1]    # first column of  Q = G_{t-1} (=delta c_t-1-mu_st-1)\n",
        "    Q[:, 1] = inputkal1[2:Nn-2, 0] - mut[2:Nn-2]    # second column of Q = G_{t-2}\n",
        "\n",
        "    # generate posterior mean & variance of phi\n",
        "    meanphi, varphi =posteriornormal(meanphip,varphip,Q.T @ Q,(Q.T @ G).reshape(-1, 1))\n",
        "\n",
        "\n",
        "    if np.isnan(varphi).any() or np.isnan(meanphi).any(): # in case NaN shows up\n",
        "       varphi = np.array([[1,0],[0,1]])\n",
        "       meanphi = np.array([[0],[0]])\n",
        "\n",
        "    # draw random sample of phi from posterior normal distribution\n",
        "    phi=np.random.multivariate_normal(meanphi.flatten(), varphi)\n",
        "\n",
        "    counter=0\n",
        "    while abs(1-sum(phi))<0.1 and counter < 100:     # avoid too big value of mu0 later: mu0=mu0*/(1-phi1-phi2)\n",
        "      phi=np.random.multivariate_normal(meanphi.flatten(), varphi)\n",
        "                                                     # if phi_1+phi_2 gets too close to 1, redo the random sampling\n",
        "      counter += 1\n",
        "\n",
        "\n",
        "# generating mu\n",
        "\n",
        "    # form G* and Q*\n",
        "    Gstar = (inputkal1[4:Nn, 0] - phi[0]* inputkal1[3:Nn - 1, 0] - phi[1] * inputkal1[2:Nn - 2, 0]).reshape(-1,1)\n",
        "    Qstar = np.ones((Nn - 4, 2))\n",
        "    Qstar[:, 1] = (St[4:Nn].flatten()\n",
        "                   - phi[0]* St[3:Nn - 1].flatten()\n",
        "                   - phi[1] * St[2:Nn - 2].flatten()).reshape(-1,)\n",
        "\n",
        "    # generate posterior variance of mu\n",
        "    meanmu, varmu = posteriornormal(meanmup,varmup, Qstar.T @ Qstar,Qstar.T @ Gstar)\n",
        "\n",
        "    # draw random sample of mu from posterior normal distribution\n",
        "    # attention, this distribution is for mu_0* and mu_1, so we need to calculate mu_0\n",
        "    # mu_0*=mu_0(1-phi1-phi2)\n",
        "    mu=np.random.multivariate_normal(meanmu.flatten(), varmu)\n",
        "    mu0=(mu[0]/(1 -np.sum(phi)))\n",
        "    mu1=mu[1]\n",
        "\n",
        "    # mu1 is mean of expansion regime, we need mu1 > 0,\n",
        "    # if mu1 < 0, we redo the random sampling again until mu1 >0\n",
        "    counter=0\n",
        "    while mu1 <= 0 and counter < 100:\n",
        "      mu=np.random.multivariate_normal(meanmu.flatten(), varmu)\n",
        "      mu0=(mu[0]/(1 -np.sum(phi)))\n",
        "      mu1=mu[1]\n",
        "\n",
        "      counter += 1\n",
        "    if mu1 <= 0:\n",
        "      mu1 = 0.5\n",
        "\n",
        "    # form mu_st (this will be used in generating G and Q in next iteration)\n",
        "    mut =( mu0 * np.ones(Nn)\n",
        "        + mu1 * np.ones(Nn) * St.flatten().flatten()\n",
        "    )\n",
        "\n",
        "########################################################################\n",
        "# main part of MCMC finishes here\n",
        "########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "########################################################################\n",
        "# generate delta\n",
        "# run kalman filter based on equation 10 and 11 in K&N (1998) to calculate delta\n",
        "# From this Kalman filter we only need one thing: Kalman Gain K, then we use K to generate delta\n",
        "\n",
        "    A = np.block([[phi[0], phi[1], 0, 0, *np.zeros(8)],\n",
        "              [1, 0, 0, 0, *np.zeros(8)],\n",
        "              [0, 1, 0, 0, *np.zeros(8)],\n",
        "              [0, 0, 1, 0, *np.zeros(8)],\n",
        "              [*np.zeros(4), psi[0,0], psi[0,1], *np.zeros(6)],\n",
        "              [*np.zeros(4), 1, *np.zeros(7)],\n",
        "              [*np.zeros(6), psi[1,0], psi[1,1], *np.zeros(4)],\n",
        "              [*np.zeros(6), 1, *np.zeros(5)],\n",
        "              [*np.zeros(8), psi[2,0], psi[2,1], *np.zeros(2)],\n",
        "              [*np.zeros(8), 1, *np.zeros(3)],\n",
        "              [*np.zeros(10), psi[3,0], psi[3,1]],\n",
        "              [*np.zeros(10), 1, *np.zeros(1)]])\n",
        "\n",
        "    H = np.block([[lamda[0], *np.zeros(3), 1, *np.zeros(7)],\n",
        "                  [lamda[1], *np.zeros(5), 1, *np.zeros(5)],\n",
        "                  [lamda[2], *np.zeros(7), 1, *np.zeros(3)],\n",
        "                  [lamda4[0], lamda4[1], lamda4[2], lamda4[3], *np.zeros(6), 1, *np.zeros(1)]])\n",
        "\n",
        "    Qdelta = np.zeros((12, 12))\n",
        "    Qdelta[0, 0] = var_vt\n",
        "    for j in range (4):\n",
        "        Qdelta[4+2*j, 4+2*j] = sig[j]\n",
        "\n",
        "    H = np.squeeze(H)\n",
        "    P = 0.1 * np.eye(12)# initial setting of variance matrix of xi_t\n",
        "    z = np.ones((4, N))\n",
        "    true4D = Y\n",
        "    #form new mu_st\n",
        "    St=SSt\n",
        "    mu_st = (mu0 * np.ones(N) + mu1 * np.ones(N) * St)\n",
        "    x=np.zeros((12,N))\n",
        "    phimu_st=np.zeros(N)\n",
        "\n",
        "    phimu_st = (np.array(mu_st[2:N])\n",
        "            - np.array(mu_st[1:N-1]* phi[0])\n",
        "            - np.array(mu_st[0:N-2]* phi[1]))\n",
        "    phimu_st = np.array(phimu_st)\n",
        "    phimu_st = np.insert(phimu_st, [0, 0], phimu_st[0])\n",
        "\n",
        "    Mu_st=np.zeros((12,N))\n",
        "    Mu_st[0,:]=phimu_st\n",
        "    R=np.zeros((4,4))\n",
        "\n",
        "    # initialize mean and variance at t=1\n",
        "    for j in range (3):\n",
        "        x[:,j] = np.vstack(((mu0+mu1*((1-p)/(2-p-q)))*np.ones((4,1)), np.zeros((8,1)))).flatten()\n",
        "    # Kronecker product of AAA with itself, resulting in a 144x144 matrix\n",
        "    A_kron = np.kron(A, A)\n",
        "    # Compute the inverse of the resulting matrix\n",
        "    inverse_matrix = np.linalg.inv(np.eye(144) - A_kron)\n",
        "    # Vectorize Q matrix in column-major order (Fortran-style)\n",
        "    Q_flat = Qdelta.flatten('F')\n",
        "    # Multiply the inverse matrix by the vectorized QQ\n",
        "    t_vr = inverse_matrix @ Q_flat\n",
        "    # Reshape the result to a 12x12 matrix\n",
        "    P = t_vr.reshape((12, 12), order='F')\n",
        "    true4D=kalpre[:,0:4].T\n",
        "    K=np.zeros(H.T.shape)\n",
        "\n",
        "#start kalman filter\n",
        "    K,x,vct=Kalman(R,H,A,Qdelta,true4D,Mu_st,N,P,K)\n",
        "##############from above, we get steady-state Kalman Gain: K\n",
        "    newdelta=(np.linalg.inv(np.eye(12)-(np.eye(12) - K @ H)@A) @ K  @ delta_y_mean)[0,0]\n",
        "\n",
        "\n",
        "\n",
        "# save the sample of posterior values (mean, variance, sigma^2_i, transition probabilties, delta c_t, delta)\n",
        "    if i>burn-1:\n",
        "      for m in range(3):\n",
        "        mlamda[i-burn,m]=meanlamda[m]\n",
        "        vlamda[i-burn,m]=varlamda[m]\n",
        "      for m in range(4):\n",
        "        mlamda[i-burn,m+3]=meanlamda4[m]\n",
        "        vlamda[i-burn,m+3]=varlamda4[m,m]\n",
        "        bsig[i-burn,m]=sig[m]\n",
        "        mpsi[i-burn,2*m]=mmeanpsi[0,m]\n",
        "        mpsi[i-burn,2*m+1]=mmeanpsi[1,m]\n",
        "        vpsi[1-burn,2*m]=vvarpsi[0,m]\n",
        "        vpsi[1-burn,2*m+1]=vvarpsi[1,m]\n",
        "      for m in range (2):\n",
        "        mphi[i-burn,m] = meanphi[m]\n",
        "        vphi[i-burn,m] = varphi[m, m]\n",
        "      mmu1[i - burn] = meanmu[0,0]/(1-sum(phi))\n",
        "      mmu2[i - burn] = meanmu[1, 0]\n",
        "      vmu1[i - burn] = varmu[0, 0]\n",
        "      vmu2[i - burn] = varmu[1, 1]\n",
        "      transition_prob_p[i-burn]=res.params[0]      #### this is p: expansion to expansion P(S_t=1|S_t-1=1)\n",
        "      transition_prob_q[i-burn]=1-res.params[1]    #### this is q: recession to recession P(S_t=0|S_t-1=0)\n",
        "      delta[i-burn]=newdelta\n",
        "      sample_ct=sample_ct+new_delta_ct\n",
        "      regime=regime+res.smoothed_marginal_probabilities[1].values.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "HkgF_8wraMLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final output of parameter distributions**"
      ],
      "metadata": {
        "id": "zipUll-iaZAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sample_ct_mean = np.mean(sample_ct) # If sample_ct is a scalar, you don't need to calculate the mean\n",
        "recession = regime/(iter-burn)\n",
        "\n",
        "# Print the mean values\n",
        "print(\"                  Mean                   \", \"SD\")\n",
        "print(\"delta c_t\")\n",
        "print(\"phi:\",  np.mean(mphi),np.sqrt(np.mean(vphi)))\n",
        "print(\"mu1:\", np.mean(mmu1), np.sqrt(np.mean(vmu1)))\n",
        "print(\"mu2:\", np.mean(mmu2), np.sqrt(np.mean(vmu2)))\n",
        "print(\"p:\", np.mean(transition_prob_p),np.sqrt(np.var(transition_prob_p)))\n",
        "print(\"q:\", np.mean(transition_prob_q),np.sqrt(np.var(transition_prob_q)))\n",
        "print(\"delta:\", np.mean(delta), np.sqrt(np.var(delta)))\n",
        "\n",
        "for i in range (4):\n",
        "    print(\"Y_\",i+1 )\n",
        "    if i<3:\n",
        "        print(\"lamda_\",i+1,\":\", np.mean(mlamda[:,i]), np.sqrt(np.mean(vlamda[:,i])))\n",
        "    else:\n",
        "        for j in range(4):\n",
        "            print(\"lamda_4\",j,\":\", np.mean(mlamda[:,j+3]), np.sqrt(np.mean(vlamda[:,j+3])))\n",
        "    print(\"sigma_\",i+1,\":\", np.mean(bsig[:,1]), np.sqrt(np.var(bsig[:,i])))\n",
        "    print(\"psi_\",i+1,\"1:\", np.mean(mpsi[:,2*i]),np.sqrt(np.mean(vpsi[:,2*i])))\n",
        "    print(\"psi_\",i+1,\"2:\", np.mean(mpsi[:,2*i+1]),np.sqrt(np.mean(vpsi[:,2*i+1])))\n",
        "\n"
      ],
      "metadata": {
        "id": "I1-01z-IaEey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "####### plot of recession probability\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Create a new DataFrame to store the ratios\n",
        "date_range_ratios = pd.date_range(start='1960-01', end='1995-01', freq='M')\n",
        "df_ratios = pd.DataFrame({'date': date_range_ratios, 'ratio': recession.ravel()})\n",
        "df_ratios.set_index('date', inplace=True)\n",
        "\n",
        "# Plot the ratio series\n",
        "df_ratios['ratio'].plot(title=\"Recession\", figsize=(5, 3))\n",
        "plt.ylim(0, 1)\n",
        "ticks = pd.date_range('1960-01-01', '1995-01-01', freq='5Y')\n",
        "plt.xticks(ticks, ticks.strftime('%Y'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Zz1lc8QaOGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "####### plot of recession probability (no smooth process)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Create a new DataFrame to store the ratios\n",
        "date_range_ratios = pd.date_range(start='1960-01', end='1995-01', freq='M')\n",
        "df_ratios = pd.DataFrame({'date': date_range_ratios, 'ratio':res.filtered_marginal_probabilities[1].ravel()})\n",
        "df_ratios.set_index('date', inplace=True)\n",
        "\n",
        "# Plot the ratio series\n",
        "df_ratios['ratio'].plot(title=\"Recession without smooth process\", figsize=(5, 3))\n",
        "ticks = pd.date_range('1960-01-01', '1995-01-01', freq='5Y')\n",
        "plt.xticks(ticks, ticks.strftime('%Y'))\n",
        "# plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k4advsBVaQcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import normal\n",
        "N=kalpre.shape[0]\n",
        "ddelta=np.mean(delta)\n",
        "C0=40\n",
        "newCLI = np.zeros((N, 1))\n",
        "dddelta=np.mean(delta)\n",
        "newCLI[0]=C0+(sample_ct/(iter-burn))[0]+dddelta\n",
        "for i in range(1, N):\n",
        "    newCLI[i] = newCLI[i-1] + (sample_ct/(iter-burn))[i]+dddelta # new delta_C_t=delta_c_t+C_{t-1}+delta according to appendix A7\n",
        "    # Create an array of monthly dates\n",
        "newCLI=newCLI[3:N]\n",
        "dates = pd.date_range(start='1960-01', end='1995-01', freq='MS')\n",
        "\n",
        "# Create a pandas Series with the data and the dates as index\n",
        "series1 = pd.Series(data=newCLI.ravel(), index=dates)\n",
        "fig, ax = plt.subplots(figsize=(5,3))\n",
        "# Plot the series\n",
        "series1.plot(figsize=(4,3), legend=False,label='new economic index',color='orange')\n",
        "\n",
        "\n",
        "# Set the y-axis limits\n",
        "#ax.set_ylim(30, 120)\n",
        "plt.legend()\n",
        "ticks = pd.date_range('1960-01-01', '2000-01-01', freq='5Y')\n",
        "plt.xticks(ticks, ticks.strftime('%Y'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qvJzHjXxaU01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}